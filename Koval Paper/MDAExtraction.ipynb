{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDA Extraction Test 1 \n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import traceback\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from yahooquery import Ticker\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "import requests\n",
    "\n",
    "\"IFQWK03BBAJDH85H\"\n",
    "\n",
    "# Define the path for the log file\n",
    "log_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output'\n",
    "log_file = os.path.join(log_directory, 'mda_extraction.log')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_file,\n",
    "    filemode='w'  # 'w' mode overwrites the file, use 'a' for append\n",
    ")\n",
    "\n",
    "# Add a StreamHandler for tqdm to work correctly\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.ERROR)  # Only show ERROR level logs in the console\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Define Regular Expressions\n",
    "# ================================\n",
    "\n",
    "\n",
    "#MDA Start Patterns\n",
    "ITEM7_PATTERNS = [\n",
    "    r'\\b(?:Item|ITEM)\\s*7\\b(?!A|\\.A)\\.?(?:\\s*[-–—])?\\s*(?:Management[\\'\\s]?s?|Managements?|Managment|Manag[ae]?ment[\\'\\s]?s?)\\b'\n",
    "]\n",
    "\n",
    "ITEM7_REGEX = re.compile('|'.join(ITEM7_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# MDA End Patterns\n",
    "ITEM7A_OR_8_PATTERNS = [\n",
    "    r'\\bItem\\s*7A\\.?\\s*',\n",
    "    r'\\bITEM\\s*7A\\.?\\s*',\n",
    "    r'\\bItem\\s*7A\\.?\\s*Quantitative\\s+and\\s+Qualitative\\s+Disclosures\\s+About\\s+Market\\s+Risk\\b',\n",
    "    r'\\bITEM\\s*7A\\.?\\s*Quantitative\\s+and\\s+Qualitative\\s+Disclosures\\s+About\\s+Market\\s+Risk\\b',\n",
    "    r'\\bItem\\s*8\\.?\\s*',\n",
    "    r'\\bITEM\\s*8\\.?\\s*',\n",
    "]\n",
    "ITEM7A_OR_8_REGEX = re.compile('|'.join(ITEM7A_OR_8_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Helper Functions\n",
    "# ================================\n",
    "\n",
    "# Dictionary to store CIK to ticker mappings\n",
    "cik_ticker_cache = {}\n",
    "\n",
    "def cik_to_ticker(cik):\n",
    "    if cik in cik_ticker_cache:\n",
    "        return cik_ticker_cache[cik]\n",
    "    \n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik.zfill(10)}.json\"\n",
    "    headers = {\n",
    "        'User-Agent': 'FinResearch/1.0 (Contact: abbraga04@gmail.com)'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    time.sleep(1)  # Add a delay of 1 second between requests\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        tickers = data.get('tickers', [])\n",
    "        cik_ticker_cache[cik] = tickers\n",
    "        return tickers\n",
    "    else:\n",
    "        cik_ticker_cache[cik] = None\n",
    "        return None\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Alpha Vantage API key\n",
    "API_KEY = 'YOUR_ALPHA_VANTAGE_API_KEY'  # Replace with your actual API key\n",
    "\n",
    "def get_stock_data(cik, start_date, end_date, max_retries=5, delay=12):\n",
    "    \"\"\"\n",
    "    Fetch daily stock price data for a given CIK and date range using Alpha Vantage.\n",
    "    \"\"\"\n",
    "    tickers = cik_to_ticker(str(cik))\n",
    "    if not tickers:\n",
    "        print(f\"No ticker found for CIK: {cik}\")\n",
    "        return pd.Series()\n",
    "\n",
    "    ticker = tickers[0]  # Use the first ticker if multiple are returned\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={ticker}&apikey={API_KEY}&outputsize=full'\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'Time Series (Daily)' in data:\n",
    "                df = pd.DataFrame(data['Time Series (Daily)']).T\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                df = df.sort_index()\n",
    "                df = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "                \n",
    "                if not df.empty:\n",
    "                    return df['4. close'].astype(float)\n",
    "                else:\n",
    "                    print(f\"No data found for ticker {ticker} (CIK: {cik}) in the specified date range\")\n",
    "            else:\n",
    "                print(f\"No data found for ticker {ticker} (CIK: {cik})\")\n",
    "            \n",
    "            return pd.Series()\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for CIK {cik}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Max retries reached for CIK {cik}\")\n",
    "                return pd.Series()\n",
    "\n",
    "def prepare_risk_prediction_data(results):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for risk prediction.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    print(f\"Total results to process: {len(results)}\")\n",
    "    \n",
    "    for result in results:\n",
    "        cik = result['cik_number']\n",
    "        company_name = result['company_name']\n",
    "        filing_date = result.get('filing_date')\n",
    "        \n",
    "        if not filing_date:\n",
    "            print(f\"Missing filing date for {company_name} (CIK: {cik}). Skipping this report.\")\n",
    "            continue\n",
    "        \n",
    "        end_date = (datetime.strptime(filing_date, '%Y-%m-%d') + timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "        print(f\"Fetching stock data for {company_name} (CIK: {cik}) from {filing_date} to {end_date}\")\n",
    "        \n",
    "        stock_prices = get_stock_data(cik, filing_date, end_date)\n",
    "        \n",
    "        if not stock_prices.empty:\n",
    "            mdd = calculate_mdd(stock_prices)\n",
    "            print(f\"Calculated MDD for {company_name} (CIK: {cik}): {mdd}\")\n",
    "            data.append({\n",
    "                'cik': cik,\n",
    "                'company_name': company_name,\n",
    "                'mda_content': result.get('mda_content', ''),\n",
    "                'mdd': mdd,\n",
    "                'filing_date': filing_date\n",
    "            })\n",
    "        else:\n",
    "            print(f\"No valid stock price data found for {company_name} (CIK: {cik})\")\n",
    "\n",
    "    print(f\"Total data points collected: {len(data)}\")\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to process for risk prediction.\")\n",
    "        return []\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['year'] = pd.to_datetime(df['filing_date']).dt.year\n",
    "    \n",
    "    def label_risk_by_year(group):\n",
    "        mdds = group['mdd'].values\n",
    "        return label_risk(mdds)\n",
    "    \n",
    "    df['risk_label'] = df.groupby('year').apply(label_risk_by_year).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Risk labels assigned. Final number of data points: {len(df)}\")\n",
    "    \n",
    "    return df.to_dict('records')\n",
    "    \n",
    "\n",
    "def contains_other_items(mda_content):\n",
    "    \"\"\"\n",
    "    Checks if the MDA content contains mentions of items other than Item 7.\n",
    "\n",
    "    Parameters:\n",
    "    - mda_content (str): The content of the MDA section.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if other items are mentioned, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # This pattern matches \"Item\" or \"ITEM\" followed by a number that's not 7\n",
    "    pattern = r'\\b(?:Item|ITEM)\\s+(?!7\\b)\\d+'\n",
    "    return bool(re.search(pattern, mda_content))\n",
    "\n",
    "def extract_company_name(content):\n",
    "    \"\"\"\n",
    "    Extracts the company name from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted company name or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)$', content, re.MULTILINE)\n",
    "    return match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "def extract_filing_date(content):\n",
    "    \"\"\"\n",
    "    Extracts the filing date from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted filing date in YYYY-MM-DD format or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'FILED AS OF DATE:\\s*(\\d{8})', content)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_company_id(content):\n",
    "    \"\"\"\n",
    "    Extracts the company identifier (CIK) from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The content of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: Extracted CIK or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Look for the CIK in the header of the file\n",
    "    cik_pattern = r'CENTRAL INDEX KEY:\\s*(\\d{10})'\n",
    "    match = re.search(cik_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # If not found in the standard location, try an alternative pattern\n",
    "    alt_pattern = r'CIK=(\\d{10})'\n",
    "    match = re.search(alt_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def count_companies(content):\n",
    "    \"\"\"\n",
    "    Counts the number of unique companies in the SEC filing based on CIK numbers,\n",
    "    company names, and filer identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of unique companies detected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract CIK numbers using the existing helper function\n",
    "    cik_number = extract_company_id(content)\n",
    "    cik_numbers = set([cik_number]) if cik_number else set()\n",
    "\n",
    "    # Extract company names using the existing helper function\n",
    "    company_name = extract_company_name(content)\n",
    "    company_names = set([company_name]) if company_name != \"Unknown\" else set()\n",
    "\n",
    "    # Search for filer identifiers\n",
    "    filer_pattern = r'FILER:\\s*\\n\\s*COMPANY DATA:'\n",
    "    filer_count = len(re.findall(filer_pattern, content))\n",
    "\n",
    "    # Determine the number of unique companies\n",
    "    num_companies = max(len(cik_numbers), len(company_names), filer_count)\n",
    "\n",
    "    return num_companies\n",
    "\n",
    "# Filter through table of contents\n",
    "def is_table_of_contents(text, strict=True):\n",
    "    \"\"\"\n",
    "    Determines if the given text is likely to be a table of contents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to analyze.\n",
    "    - strict (bool): Whether to use strict or lenient criteria.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the text is likely a table of contents, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Checking if the text is a table of contents...\")\n",
    "    # Check if the text is short\n",
    "    if len(text.split()) < 200:\n",
    "        logging.info(\"Text is short, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for patterns typical in table of contents\n",
    "    toc_patterns = [\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\.\\s+.*\\d+\\s*$',  # Item followed by number and page number\n",
    "        r'\\bTable\\s+of\\s+Contents\\b',\n",
    "        r'\\b(Page|p\\.)\\s+\\d+\\b',\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\..*\\n.*\\d+\\s*$',  # Item title followed by page number on next line\n",
    "        r'^\\s*\\d+\\s*$',  # Standalone page numbers\n",
    "        r'\\bItem\\s+\\d+[A-Z]?.*\\d+\\s*\\b',  # Item followed by any text and a number (page number)\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\.?\\s+[^.]+?(?=\\s+Item|\\s+\\d+|$)'  # Item followed by title, ending at next Item or number\n",
    "    ]\n",
    "    \n",
    "    toc_regex = re.compile('|'.join(toc_patterns), re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Count matches\n",
    "    matches = toc_regex.findall(text)\n",
    "    if strict and len(matches) > 3:  # If more than 3 matches, likely a table of contents\n",
    "        logging.info(f\"Detected {len(matches)} matches in strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    elif not strict and len(matches) > 5:  # Less strict for longer sections\n",
    "        logging.info(f\"Detected {len(matches)} matches in less strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for consecutive item listings\n",
    "    items = re.findall(r'\\bItem\\s+\\d+[A-Z]?', text, re.IGNORECASE)\n",
    "    if len(items) > 2:  # More than 2 ITEM listings\n",
    "        # Check if these items appear close to each other\n",
    "        item_positions = [m.start() for m in re.finditer(r'\\bItem\\s+\\d+[A-Z]?', text, re.IGNORECASE)]\n",
    "        if len(item_positions) > 1:\n",
    "            avg_distance = sum(item_positions[i+1] - item_positions[i] for i in range(len(item_positions)-1)) / (len(item_positions)-1)\n",
    "            if strict and avg_distance < 200:  # If average distance between ITEMs is less than 200 characters\n",
    "                logging.info(\"High density of item listings detected, likely a table of contents.\")\n",
    "                return True\n",
    "            elif not strict and avg_distance < 100:  # Less strict for longer sections\n",
    "                logging.info(\"High density of item listings detected in less strict mode, likely a table of contents.\")\n",
    "                return True\n",
    "    \n",
    "    # Check for high density of item listings\n",
    "    words = text.split()\n",
    "    item_density = len(items) / len(words)\n",
    "    if strict and item_density > 0.01:  # If more than 1% of words are item listings, likely a table of contents\n",
    "        logging.info(\"Item density exceeds 1%, likely a table of contents.\")\n",
    "        return True\n",
    "    elif not strict and item_density > 0.02:  # Less strict for longer sections\n",
    "        logging.info(\"Item density exceeds 2% in less strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for short paragraphs (typical in TOC)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    short_paragraphs = [p for p in paragraphs if len(p.split()) < 20]\n",
    "    \n",
    "    if len(short_paragraphs) / len(paragraphs) > 0.5:  # If more than half of paragraphs are short, likely a TOC\n",
    "        logging.info(\"More than half of paragraphs are short, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    logging.info(\"Text does not appear to be a table of contents.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def pair_reports(results):\n",
    "    \"\"\"\n",
    "    Pairs reports for companies based on their filing dates, allowing for delays and differences in publication dates.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): Extracted MDA sections with filenames and filing dates.\n",
    "\n",
    "    Returns:\n",
    "    - list of dict: Paired reports.\n",
    "    \"\"\"\n",
    "    # Group reports by company\n",
    "    company_reports = {}\n",
    "    for report in results:\n",
    "        cik = report['cik_number']\n",
    "        if cik not in company_reports:\n",
    "            company_reports[cik] = []\n",
    "        company_reports[cik].append(report)\n",
    "\n",
    "    paired_reports = []\n",
    "    for cik, reports in company_reports.items():\n",
    "        # Sort reports by filing date\n",
    "        sorted_reports = sorted(reports, key=lambda x: datetime.strptime(x['filing_date'], '%Y-%m-%d'))\n",
    "        \n",
    "        for i in range(len(sorted_reports) - 1):\n",
    "            current_report = sorted_reports[i]\n",
    "            next_report = sorted_reports[i + 1]\n",
    "            \n",
    "            current_date = datetime.strptime(current_report['filing_date'], '%Y-%m-%d')\n",
    "            next_date = datetime.strptime(next_report['filing_date'], '%Y-%m-%d')\n",
    "            \n",
    "            # Check if reports are within a reasonable time frame (e.g., 9-15 months apart)\n",
    "            if timedelta(days=270) <= next_date - current_date <= timedelta(days=450):\n",
    "                paired_reports.append({\n",
    "                    'current_filename': current_report['filename'],\n",
    "                    'next_filename': next_report['filename'],\n",
    "                    'company_name': current_report['company_name'],\n",
    "                    'cik_number': cik,\n",
    "                    'current_filing_date': current_report['filing_date'],\n",
    "                    'next_filing_date': next_report['filing_date'],\n",
    "                    'current_mda_content': current_report['mda_content'],\n",
    "                    'next_mda_content': next_report['mda_content'],\n",
    "                    'time_difference': (next_date - current_date).days\n",
    "                })\n",
    "\n",
    "    return paired_reports\n",
    "\n",
    "def pair_and_save_reports(results, output_directory):\n",
    "    \"\"\"\n",
    "    Pairs reports and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): Extracted MDA sections with filenames and filing dates.\n",
    "    - output_directory (str): Directory to save the paired reports CSV.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    paired_reports_csv = os.path.join(output_directory, 'paired_mda_reports.csv')\n",
    "    \n",
    "    # Use the new pair_reports function that doesn't rely on fiscal_calendar_df\n",
    "    paired_reports = pair_reports(results)\n",
    "    \n",
    "    if paired_reports:\n",
    "        fieldnames = ['current_filename', 'next_filename', 'company_name', 'cik_number', \n",
    "                      'current_filing_date', 'next_filing_date', 'current_mda_content', \n",
    "                      'next_mda_content', 'time_difference']\n",
    "        \n",
    "        with open(paired_reports_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in paired_reports:\n",
    "                writer.writerow(row)\n",
    "        logging.info(f\"INFO: Paired reports saved to {paired_reports_csv}\")\n",
    "    else:\n",
    "        logging.warning(\"WARNING: No paired reports found.\")\n",
    "\n",
    "# ================================\n",
    "# 2. MDA Text Extraction\n",
    "# ================================\n",
    "\n",
    "\n",
    "def extract_mda_section(text):\n",
    "    \"\"\"\n",
    "    Extracts the Management's Discussion and Analysis (MDA) section from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary containing the extracted MDA section and metadata, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Normalizing text for extraction...\")\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    item7_matches = list(ITEM7_REGEX.finditer(normalized_text))\n",
    "    logging.info(f\"Found {len(item7_matches)} potential Item 7 matches.\")\n",
    "    \n",
    "    if not item7_matches:\n",
    "        logging.info(\"No Item 7 matches found.\")\n",
    "        return None\n",
    "    \n",
    "    for start_match in item7_matches:\n",
    "        start_index = start_match.start()\n",
    "        start_pattern = start_match.group()\n",
    "        logging.info(f\"Found potential MDA start at index {start_index}: '{start_pattern}'\")\n",
    "        \n",
    "        if 'item 6' in start_pattern.lower() and 'management' not in start_pattern.lower() and 'md&a' not in start_pattern.lower():\n",
    "            logging.info(\"Skipping Item 6 as it is not explicitly an MDA.\")\n",
    "            continue\n",
    "        \n",
    "        # Find end of MDA\n",
    "        end_match = ITEM7A_OR_8_REGEX.search(normalized_text[start_index + 100:])\n",
    "        if end_match:\n",
    "            end_index = start_index + 100 + end_match.start()\n",
    "            logging.info(f\"Found potential MDA end at index {end_index}.\")\n",
    "        else:\n",
    "            end_index = len(normalized_text)\n",
    "            logging.info(\"No end pattern found, using end of document.\")\n",
    "        \n",
    "        mda_text = normalized_text[start_index:end_index]\n",
    "        \n",
    "        # Check if the entire section is a table of contents\n",
    "        if is_table_of_contents(mda_text):\n",
    "            logging.info(\"This section appears to be a table of contents. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # If the beginning looks like a table of contents, try to find the real start of the MDA\n",
    "        first_1000_words = ' '.join(mda_text.split()[:1000])\n",
    "        if is_table_of_contents(first_1000_words):\n",
    "            logging.info(\"The beginning looks like a table of contents. Searching for the real MDA start.\")\n",
    "            real_start_match = re.search(r'(Management\\'s\\s+Discussion\\s+and\\s+Analysis|MD&A).{0,500}?(?=\\n\\n)', mda_text, re.IGNORECASE | re.DOTALL)\n",
    "            if real_start_match:\n",
    "                start_index += real_start_match.start()\n",
    "                mda_text = mda_text[real_start_match.start():]\n",
    "                logging.info(f\"Found real MDA start at index {start_index}.\")\n",
    "            else:\n",
    "                logging.info(\"Couldn't find the real MDA start. Skipping this occurrence.\")\n",
    "                continue\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(mda_text.split())\n",
    "        logging.info(f\"Extracted MDA section with word count: {word_count}.\")\n",
    "        if 500 <= word_count:\n",
    "            return {\n",
    "                'mda_content': mda_text,\n",
    "                'start_index': start_index,\n",
    "                'end_index': end_index,\n",
    "                'start_pattern': start_pattern,\n",
    "                'end_pattern': end_match.group() if end_match else \"End of document\",\n",
    "                'word_count': word_count\n",
    "            }\n",
    "    \n",
    "    logging.info(\"No suitable MDA section found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Method to process each file \n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single 10-K file to extract the MDA section and related metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the 10-K file.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary containing the extracted MDA section and metadata, or None if extraction fails.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        num_companies = count_companies(content)\n",
    "\n",
    "        if num_companies != 1:\n",
    "            logging.info(f\"Skipping file {file_path}: Detected {num_companies} companies.\")\n",
    "            return None \n",
    "\n",
    "        cik_number = extract_company_id(content)\n",
    "        company_name = extract_company_name(content)\n",
    "        filing_date = extract_filing_date(content)\n",
    "\n",
    "        # Check if we've already processed this company\n",
    "        if cik_number in cik_ticker_cache:\n",
    "            if cik_ticker_cache[cik_number] is None:\n",
    "                logging.info(f\"Skipping file {file_path}: No ticker found for company {company_name} (CIK: {cik_number}).\")\n",
    "                return None\n",
    "        else:\n",
    "            # If not, check if the company has a ticker\n",
    "            tickers = cik_to_ticker(str(cik_number))\n",
    "            if not tickers:\n",
    "                logging.info(f\"Skipping file {file_path}: No ticker found for company {company_name} (CIK: {cik_number}).\")\n",
    "                return None\n",
    "\n",
    "        result = extract_mda_section(content)\n",
    "\n",
    "        if result:\n",
    "            result.update({\n",
    "                'cik_number': cik_number,\n",
    "                'company_name': company_name,\n",
    "                'filing_date': filing_date,\n",
    "                'filename': os.path.basename(file_path)\n",
    "            })\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "        \n",
    "# ================================\n",
    "# 3. Main Processing Function\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process 10-K files, extract MDA sections, generate paired reports,\n",
    "    and prepare risk prediction data.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing filtered results and risk prediction data.\n",
    "    \"\"\"\n",
    "\n",
    "    base_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data'\n",
    "    output_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    all_results = []\n",
    "    all_files = []\n",
    "\n",
    "    # Process specified periods\n",
    "    for period in ['10-X_C_2001-2005', '10-X_C_2006-2010', '10-X_C_2011-2015', '10-X_C_2016-2020']:\n",
    "        period_path = os.path.join(base_directory, period)\n",
    "        if os.path.isdir(period_path):\n",
    "            for year in os.listdir(period_path):\n",
    "                year_path = os.path.join(period_path, year)\n",
    "                if os.path.isdir(year_path) and year.isdigit():\n",
    "                    for quarter in ['QTR1', 'QTR2', 'QTR3', 'QTR4']:\n",
    "                        quarter_path = os.path.join(year_path, quarter)\n",
    "                        if os.path.isdir(quarter_path):\n",
    "                            logging.info(f\"Processing {period} - {year} {quarter}\")\n",
    "                            quarter_files = [\n",
    "                                os.path.join(quarter_path, f)\n",
    "                                for f in os.listdir(quarter_path)\n",
    "                                if f.lower().endswith('.txt') and '10-k' in f.lower() and not any(x in f.lower() for x in ['10-k/a', '10-k-a'])\n",
    "                            ]\n",
    "                            all_files.extend(quarter_files)\n",
    "                        if (len(all_files) >= 10000):\n",
    "                            break\n",
    "                if (len(all_files) >= 10000):\n",
    "                    break\n",
    "        if (len(all_files) >= 10000):\n",
    "            break\n",
    "    \n",
    "    all_files = all_files[:10000]\n",
    "\n",
    "    # Process files concurrently\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file_path): file_path for file_path in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Processing files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "\n",
    "    # Filter out MDAs with mentions of items other than Item 7\n",
    "    filtered_results = [result for result in all_results if not contains_other_items(result['mda_content'])]\n",
    "    \n",
    "    logging.info(f\"Total MDAs extracted: {len(all_results)}\")\n",
    "    logging.info(f\"MDAs with only Item 7 mentioned: {len(filtered_results)}\")\n",
    "\n",
    "    # Save all results to a single CSV (including those with other items mentioned)\n",
    "    output_csv_file = os.path.join(output_directory, 'all_extracted_MDAs_2001-2020.csv')\n",
    "    fieldnames = ['filename', 'company_name', 'cik_number', 'filing_date', 'mda_content', 'start_index', 'end_index', 'end_pattern', 'start_pattern', 'word_count']\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_results:\n",
    "            writer.writerow(row)\n",
    "    logging.info(f\"INFO: All extracted MDA sections and metadata saved to {output_csv_file}\")\n",
    "\n",
    "    # Prepare data for risk prediction\n",
    "    risk_prediction_data = prepare_risk_prediction_data(filtered_results)\n",
    "    \n",
    "    # Save risk prediction data\n",
    "    risk_prediction_csv = os.path.join(output_directory, 'risk_prediction_data_2001-2020.csv')\n",
    "    risk_prediction_df = pd.DataFrame(risk_prediction_data)\n",
    "    risk_prediction_df.to_csv(risk_prediction_csv, index=False)\n",
    "    logging.info(f\"INFO: Risk prediction data saved to {risk_prediction_csv}\")\n",
    "\n",
    "    print(f\"Total files processed: {len(all_files)}\")\n",
    "    print(f\"Number of files with MDA sections: {len(all_results)}\")\n",
    "    print(f\"Number of MDAs with only Item 7 mentioned: {len(filtered_results)}\")\n",
    "    print(f\"Number of risk prediction data points: {len(risk_prediction_data)}\")\n",
    "\n",
    "    return filtered_results, risk_prediction_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filtered_results, risk_prediction_data = main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# MDA Extraction Test 1 \n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import traceback\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from yahooquery import Ticker\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "import requests\n",
    "\n",
    "# Define the path for the log file\n",
    "log_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output'\n",
    "log_file = os.path.join(log_directory, 'mda_extraction.log')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_file,\n",
    "    filemode='w'  # 'w' mode overwrites the file, use 'a' for append\n",
    ")\n",
    "\n",
    "# Add a StreamHandler for tqdm to work correctly\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.ERROR)  # Only show ERROR level logs in the console\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Define Regular Expressions\n",
    "# ================================\n",
    "\n",
    "\n",
    "#MDA Start Patterns\n",
    "# ITEM7_PATTERNS = [\n",
    "\n",
    "#     r'\\b(?:Item|ITEM)\\s*7\\b(?!A|\\.A)\\.?(?:\\s*[-–—])?\\s*(?:Management[\\'\\s]?s?|Managements?|Managment|Manag[ae]?ment[\\'\\s]?s?)\\b'\n",
    "# ]\n",
    "\n",
    "# ITEM7_PATTERNS = [\n",
    "#     r'(?:^|(?<=\\n))(?:\\s*(?:\\r?\\n|\\s)*)\\b(?:Item|ITEM)\\s*(?:\\r?\\n|\\s)*7\\b(?!A|\\.A)\\.?(?:\\s*[-–—])?\\s*(?:Management[\\'\\s]?s?|Managements?|Managment|Manag[ae]?ment[\\'\\s]?s?)\\b'\n",
    "# ]\n",
    "\n",
    "ITEM7_PATTERNS = [\n",
    "    r'(?:^|(?<=\\n))(?:\\s*(?:\\r?\\n|\\s)*)\\b(?:Item|ITEM)\\s*7\\b(?!A|\\.A)'\n",
    "]\n",
    "\n",
    "ITEM7_REGEX = re.compile('|'.join(ITEM7_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# MDA End Patterns\n",
    "ITEM7A_OR_8_PATTERNS = [\n",
    "    r'\\bItem\\s*7A\\.?\\s*',\n",
    "    r'\\bITEM\\s*7A\\.?\\s*',\n",
    "    r'\\bItem\\s*7A\\.?\\s*Quantitative\\s+and\\s+Qualitative\\s+Disclosures\\s+About\\s+Market\\s+Risk\\b',\n",
    "    r'\\bITEM\\s*7A\\.?\\s*Quantitative\\s+and\\s+Qualitative\\s+Disclosures\\s+About\\s+Market\\s+Risk\\b',\n",
    "    r'\\bItem\\s*8\\.?\\s*',\n",
    "    r'\\bITEM\\s*8\\.?\\s*',\n",
    "]\n",
    "ITEM7A_OR_8_REGEX = re.compile('|'.join(ITEM7A_OR_8_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Helper Functions\n",
    "# ================================\n",
    "\n",
    "def contains_other_items(mda_content):\n",
    "    \"\"\"\n",
    "    Checks if the MDA content contains mentions of items other than Item 7.\n",
    "\n",
    "    Parameters:\n",
    "    - mda_content (str): The content of the MDA section.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if other items are mentioned, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # This pattern matches \"Item\" or \"ITEM\" followed by a number that's not 7\n",
    "    pattern = r'\\b(?:Item|ITEM)\\s+(?!7\\b)\\d+'\n",
    "    return bool(re.search(pattern, mda_content))\n",
    "\n",
    "def extract_company_name(content):\n",
    "    \"\"\"\n",
    "    Extracts the company name from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted company name or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)$', content, re.MULTILINE)\n",
    "    return match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "def extract_filing_date(content):\n",
    "    \"\"\"\n",
    "    Extracts the filing date from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted filing date in YYYY-MM-DD format or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'FILED AS OF DATE:\\s*(\\d{8})', content)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_company_id(content):\n",
    "    \"\"\"\n",
    "    Extracts the company identifier (CIK) from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The content of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: Extracted CIK or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Look for the CIK in the header of the file\n",
    "    cik_pattern = r'CENTRAL INDEX KEY:\\s*(\\d{10})'\n",
    "    match = re.search(cik_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # If not found in the standard location, try an alternative pattern\n",
    "    alt_pattern = r'CIK=(\\d{10})'\n",
    "    match = re.search(alt_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def count_companies(content):\n",
    "    \"\"\"\n",
    "    Counts the number of unique companies in the SEC filing based on CIK numbers,\n",
    "    company names, and filer identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of unique companies detected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract CIK numbers using the existing helper function\n",
    "    cik_number = extract_company_id(content)\n",
    "    cik_numbers = set([cik_number]) if cik_number else set()\n",
    "\n",
    "    # Extract company names using the existing helper function\n",
    "    company_name = extract_company_name(content)\n",
    "    company_names = set([company_name]) if company_name != \"Unknown\" else set()\n",
    "\n",
    "    # Search for filer identifiers\n",
    "    filer_pattern = r'FILER:\\s*\\n\\s*COMPANY DATA:'\n",
    "    filer_count = len(re.findall(filer_pattern, content))\n",
    "\n",
    "    # Determine the number of unique companies\n",
    "    num_companies = max(len(cik_numbers), len(company_names), filer_count)\n",
    "\n",
    "    return num_companies\n",
    "\n",
    "# If found Item 7 is less than 500 words, skip it and search for a new one\n",
    "# Remove TOC Regex\n",
    "\n",
    "# Filter through table of contents\n",
    "def is_table_of_contents(text, strict=True):\n",
    "    \"\"\"\n",
    "    Determines if the given text is likely to be a table of contents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to analyze.\n",
    "    - strict (bool): Whether to use strict or lenient criteria.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the text is likely a table of contents, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Checking if the text is a table of contents...\")\n",
    "    # Check if the text is short\n",
    "    if len(text.split()) < 200:\n",
    "        logging.info(\"Text is short, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for patterns typical in table of contents\n",
    "    toc_patterns = [\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\.\\s+.*\\d+\\s*$',  # Item followed by number and page number\n",
    "        r'\\bTable\\s+of\\s+Contents\\b',\n",
    "        r'\\b(Page|p\\.)\\s+\\d+\\b',\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\..*\\n.*\\d+\\s*$',  # Item title followed by page number on next line\n",
    "        r'^\\s*\\d+\\s*$',  # Standalone page numbers\n",
    "        r'\\bItem\\s+\\d+[A-Z]?.*\\d+\\s*\\b',  # Item followed by any text and a number (page number)\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\.?\\s+[^.]+?(?=\\s+Item|\\s+\\d+|$)'  # Item followed by title, ending at next Item or number\n",
    "    ]\n",
    "    \n",
    "    toc_regex = re.compile('|'.join(toc_patterns), re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Count matches\n",
    "    matches = toc_regex.findall(text)\n",
    "    if strict and len(matches) > 3:  # If more than 3 matches, likely a table of contents\n",
    "        logging.info(f\"Detected {len(matches)} matches in strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    elif not strict and len(matches) > 5:  # Less strict for longer sections\n",
    "        logging.info(f\"Detected {len(matches)} matches in less strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for consecutive item listings\n",
    "    items = re.findall(r'\\bItem\\s+\\d+[A-Z]?', text, re.IGNORECASE)\n",
    "    if len(items) > 2:  # More than 2 ITEM listings\n",
    "        # Check if these items appear close to each other\n",
    "        item_positions = [m.start() for m in re.finditer(r'\\bItem\\s+\\d+[A-Z]?', text, re.IGNORECASE)]\n",
    "        if len(item_positions) > 1:\n",
    "            avg_distance = sum(item_positions[i+1] - item_positions[i] for i in range(len(item_positions)-1)) / (len(item_positions)-1)\n",
    "            if strict and avg_distance < 200:  # If average distance between ITEMs is less than 200 characters\n",
    "                logging.info(\"High density of item listings detected, likely a table of contents.\")\n",
    "                return True\n",
    "            elif not strict and avg_distance < 100:  # Less strict for longer sections\n",
    "                logging.info(\"High density of item listings detected in less strict mode, likely a table of contents.\")\n",
    "                return True\n",
    "    \n",
    "    # Check for high density of item listings\n",
    "    words = text.split()\n",
    "    item_density = len(items) / len(words)\n",
    "    if strict and item_density > 0.01:  # If more than 1% of words are item listings, likely a table of contents\n",
    "        logging.info(\"Item density exceeds 1%, likely a table of contents.\")\n",
    "        return True\n",
    "    elif not strict and item_density > 0.02:  # Less strict for longer sections\n",
    "        logging.info(\"Item density exceeds 2% in less strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for short paragraphs (typical in TOC)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    short_paragraphs = [p for p in paragraphs if len(p.split()) < 20]\n",
    "    \n",
    "    if len(short_paragraphs) / len(paragraphs) > 0.5:  # If more than half of paragraphs are short, likely a TOC\n",
    "        logging.info(\"More than half of paragraphs are short, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    logging.info(\"Text does not appear to be a table of contents.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def pair_reports(results):\n",
    "    \"\"\"\n",
    "    Pairs reports for companies based on their filing dates, allowing for delays and differences in publication dates.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): Extracted MDA sections with filenames and filing dates.\n",
    "\n",
    "    Returns:\n",
    "    - list of dict: Paired reports.\n",
    "    \"\"\"\n",
    "    # Group reports by company\n",
    "    company_reports = {}\n",
    "    for report in results:\n",
    "        cik = report['cik_number']\n",
    "        if cik not in company_reports:\n",
    "            company_reports[cik] = []\n",
    "        company_reports[cik].append(report)\n",
    "\n",
    "    paired_reports = []\n",
    "    for cik, reports in company_reports.items():\n",
    "        # Sort reports by filing date\n",
    "        sorted_reports = sorted(reports, key=lambda x: datetime.strptime(x['filing_date'], '%Y-%m-%d'))\n",
    "        \n",
    "        for i in range(len(sorted_reports) - 1):\n",
    "            current_report = sorted_reports[i]\n",
    "            next_report = sorted_reports[i + 1]\n",
    "            \n",
    "            current_date = datetime.strptime(current_report['filing_date'], '%Y-%m-%d')\n",
    "            next_date = datetime.strptime(next_report['filing_date'], '%Y-%m-%d')\n",
    "            \n",
    "            # Check if reports are within a reasonable time frame (e.g., 9-15 months apart)\n",
    "            if timedelta(days=270) <= next_date - current_date <= timedelta(days=450):\n",
    "                paired_reports.append({\n",
    "                    'current_filename': current_report['filename'],\n",
    "                    'next_filename': next_report['filename'],\n",
    "                    'company_name': current_report['company_name'],\n",
    "                    'cik_number': cik,\n",
    "                    'current_filing_date': current_report['filing_date'],\n",
    "                    'next_filing_date': next_report['filing_date'],\n",
    "                    'current_mda_content': current_report['mda_content'],\n",
    "                    'next_mda_content': next_report['mda_content'],\n",
    "                    'time_difference': (next_date - current_date).days\n",
    "                })\n",
    "\n",
    "    return paired_reports\n",
    "\n",
    "def pair_and_save_reports(results, output_directory):\n",
    "    \"\"\"\n",
    "    Pairs reports and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): Extracted MDA sections with filenames and filing dates.\n",
    "    - output_directory (str): Directory to save the paired reports CSV.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    paired_reports_csv = os.path.join(output_directory, 'paired_mda_reports.csv')\n",
    "    \n",
    "    # Use the new pair_reports function that doesn't rely on fiscal_calendar_df\n",
    "    paired_reports = pair_reports(results)\n",
    "    \n",
    "    if paired_reports:\n",
    "        fieldnames = ['current_filename', 'next_filename', 'company_name', 'cik_number', \n",
    "                      'current_filing_date', 'next_filing_date', 'current_mda_content', \n",
    "                      'next_mda_content', 'time_difference']\n",
    "        \n",
    "        with open(paired_reports_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in paired_reports:\n",
    "                writer.writerow(row)\n",
    "        logging.info(f\"INFO: Paired reports saved to {paired_reports_csv}\")\n",
    "    else:\n",
    "        logging.warning(\"WARNING: No paired reports found.\")\n",
    "\n",
    "# ================================\n",
    "# 2. MDA Text Extraction\n",
    "# ================================\n",
    "\n",
    "\n",
    "def extract_mda_section(text):\n",
    "    \"\"\"\n",
    "    Extracts the Management's Discussion and Analysis (MDA) section from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary containing the extracted MDA section and metadata, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Normalizing text for extraction...\")\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    item7_matches = list(ITEM7_REGEX.finditer(normalized_text))\n",
    "    print(f\"Found {len(item7_matches)} potential Item 7 matches.\")\n",
    "    \n",
    "    if not item7_matches:\n",
    "        logging.info(\"No Item 7 matches found.\")\n",
    "        return None\n",
    "    \n",
    "    for start_match in item7_matches:\n",
    "        start_index = start_match.start()\n",
    "        start_pattern = start_match.group()\n",
    "        logging.info(f\"Found potential MDA start at index {start_index}: '{start_pattern}'\")\n",
    "        \n",
    "        if 'item 6' in start_pattern.lower() and 'management' not in start_pattern.lower() and 'md&a' not in start_pattern.lower():\n",
    "            logging.info(\"Skipping Item 6 as it is not explicitly an MDA.\")\n",
    "            continue\n",
    "        \n",
    "        # Find end of MDA\n",
    "        end_match = ITEM7A_OR_8_REGEX.search(normalized_text[start_index + 100:])\n",
    "        if end_match:\n",
    "            end_index = start_index + 100 + end_match.start()\n",
    "            logging.info(f\"Found potential MDA end at index {end_index}.\")\n",
    "        else:\n",
    "            end_index = len(normalized_text)\n",
    "            logging.info(\"No end pattern found, using end of document.\")\n",
    "        \n",
    "        mda_text = normalized_text[start_index:end_index]\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(mda_text.split())\n",
    "        logging.info(f\"Extracted MDA section with word count: {word_count}.\")\n",
    "        if 500 <= word_count:\n",
    "            return {\n",
    "                'mda_content': mda_text,\n",
    "                'start_index': start_index,\n",
    "                'end_index': end_index,\n",
    "                'start_pattern': start_pattern,\n",
    "                'end_pattern': end_match.group() if end_match else \"End of document\",\n",
    "                'word_count': word_count\n",
    "            }\n",
    "    \n",
    "    logging.info(\"No suitable MDA section found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Method to process each file \n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single 10-K file to extract the MDA section and related metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the 10-K file.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary containing the extracted MDA section and metadata, or None if extraction fails.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        num_companies = count_companies(content)\n",
    "\n",
    "        if num_companies != 1:\n",
    "            logging.info(f\"Skipping file {file_path}: Detected {num_companies} companies.\")\n",
    "            return None \n",
    "\n",
    "        cik_number = extract_company_id(content)\n",
    "        company_name = extract_company_name(content)\n",
    "        filing_date = extract_filing_date(content)\n",
    "\n",
    "        result = extract_mda_section(content)\n",
    "\n",
    "        if result:\n",
    "            result.update({\n",
    "                'cik_number': cik_number,\n",
    "                'company_name': company_name,\n",
    "                'filing_date': filing_date,\n",
    "                'filename': os.path.basename(file_path)\n",
    "            })\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "        \n",
    "# ================================\n",
    "# 3. Main Processing Function\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process 10-K files, extract MDA sections, generate paired reports,\n",
    "    and prepare risk prediction data.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing filtered results and risk prediction data.\n",
    "    \"\"\"\n",
    "\n",
    "    base_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data'\n",
    "    output_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    all_results = []\n",
    "    all_files = []\n",
    "\n",
    "    # Process specified periods\n",
    "    for period in ['10-X_C_2001-2005', '10-X_C_2006-2010', '10-X_C_2011-2015', '10-X_C_2016-2020']:\n",
    "        period_path = os.path.join(base_directory, period)\n",
    "        if os.path.isdir(period_path):\n",
    "            for year in os.listdir(period_path):\n",
    "                year_path = os.path.join(period_path, year)\n",
    "                if os.path.isdir(year_path) and year.isdigit():\n",
    "                    for quarter in ['QTR1', 'QTR2', 'QTR3', 'QTR4']:\n",
    "                        quarter_path = os.path.join(year_path, quarter)\n",
    "                        if os.path.isdir(quarter_path):\n",
    "                            logging.info(f\"Processing {period} - {year} {quarter}\")\n",
    "                            quarter_files = [\n",
    "                                os.path.join(quarter_path, f)\n",
    "                                for f in os.listdir(quarter_path)\n",
    "                                if f.lower().endswith('.txt') and '10-k' in f.lower() and not any(x in f.lower() for x in ['10-k/a', '10-k-a'])\n",
    "                            ]\n",
    "                            all_files.extend(quarter_files)\n",
    "                        if (len(all_files) >= 100):\n",
    "                            break\n",
    "                if (len(all_files) >= 100):\n",
    "                    break\n",
    "        if (len(all_files) >= 100):\n",
    "            break\n",
    "    \n",
    "    all_files = all_files[:100]\n",
    "\n",
    "    # Process files concurrently\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file_path): file_path for file_path in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Processing files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "\n",
    "    # Filter out MDAs with mentions of items other than Item 7\n",
    "    filtered_results = [result for result in all_results if not contains_other_items(result['mda_content'])]\n",
    "    \n",
    "    logging.info(f\"Total MDAs extracted: {len(all_results)}\")\n",
    "    logging.info(f\"MDAs with only Item 7 mentioned: {len(filtered_results)}\")\n",
    "\n",
    "    # Save all results to a single CSV (including those with other items mentioned)\n",
    "    output_csv_file = os.path.join(output_directory, 'all_extracted_MDAs_2001-2020.csv')\n",
    "    fieldnames = ['filename', 'company_name', 'cik_number', 'filing_date', 'mda_content', 'start_index', 'end_index', 'end_pattern', 'start_pattern', 'word_count']\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_results:\n",
    "            writer.writerow(row)\n",
    "    logging.info(f\"INFO: All extracted MDA sections and metadata saved to {output_csv_file}\")\n",
    "\n",
    "\n",
    "    print(f\"Total files processed: {len(all_files)}\")\n",
    "    print(f\"Number of files with MDA sections: {len(all_results)}\")\n",
    "    print(f\"Number of MDAs with only Item 7 mentioned: {len(filtered_results)}\")\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filtered_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Define the management pattern\n",
    "management_pattern = re.compile(r'ITEM\\s+7\\.?\\s+MANAGEMENT\\'?\\s*S?(.*?)ITEM\\s+[7A|7(a)|8]', re.IGNORECASE|re.DOTALL|re.MULTILINE)\n",
    "\n",
    "# Define a pattern to check for other items\n",
    "other_items_pattern = re.compile(r'\\b(?:Item|ITEM)\\s+(?!7\\b)\\d+', re.IGNORECASE)\n",
    "\n",
    "def contains_other_items(mda_content):\n",
    "    \"\"\"\n",
    "    Checks if the MDA content contains mentions of items other than Item 7 more than two times.\n",
    "\n",
    "    Parameters:\n",
    "    - mda_content (str): The content of the MDA section.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if other items are mentioned more than twice, False otherwise.\n",
    "    \"\"\"\n",
    "    matches = other_items_pattern.findall(mda_content)\n",
    "    return len(matches) > 2\n",
    "\n",
    "def check_management_pattern(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        # Check for management pattern using findall\n",
    "        management_matches = management_pattern.findall(content)\n",
    "        \n",
    "        # Filter out matches with less than 500 words and check for other items\n",
    "        valid_matches = []\n",
    "        for match in management_matches:\n",
    "            word_count = len(match.split())\n",
    "            if word_count >= 500 and not contains_other_items(match):\n",
    "                valid_matches.append((match, word_count))\n",
    "        \n",
    "        if not valid_matches:\n",
    "            return None\n",
    "        \n",
    "        # Get the longest match\n",
    "        longest_match = max(valid_matches, key=lambda x: x[1])\n",
    "        \n",
    "        # Get 50 words before the longest match\n",
    "        words_before = ' '.join(content.split()[:content.index(longest_match[0])]).split()[-50:]\n",
    "        prefix = ' '.join(words_before)\n",
    "        \n",
    "        return {\n",
    "            'file_path': file_path,\n",
    "            'mda_content': prefix + ' ' + longest_match[0],\n",
    "            'word_count': longest_match[1]\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    base_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data'\n",
    "    output_file = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\longest_mdas.csv'\n",
    "    \n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt') and ('10-k' in file.lower() or '10-k405' in file.lower()) and not any(x in file.lower() for x in ['10-k/a', '10-k-a', '10-k405-a']):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(check_management_pattern, file_path): file_path for file_path in all_files}\n",
    "        \n",
    "        all_results = []\n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Processing files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "\n",
    "    # Write longest MDAs to CSV file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['file_path', 'word_count', 'mda_content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for result in all_results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "    print(f\"Total files processed: {len(all_files)}\")\n",
    "    print(f\"Files with valid management pattern: {len(all_results)}\")\n",
    "    print(f\"Longest MDAs saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "base_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data'\n",
    "output_file = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\cik_ticker_mapping.json'\n",
    "\n",
    "# Initialize the caches as global variables\n",
    "cik_ticker_cache = {}\n",
    "processed_ciks = set()\n",
    "\n",
    "def get_sec_tickers():\n",
    "    \"\"\"Get the complete CIK to ticker mapping including exchange data from SEC\"\"\"\n",
    "    url = \"https://www.sec.gov/files/company_tickers_exchange.json\"\n",
    "    headers = {'User-Agent': 'FinResearch/1.0 (Contact: abbraga04@gmail.com)'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            mapping = {}\n",
    "            for entry in data['data']:\n",
    "                cik = str(entry[0]).zfill(10)  # CIK\n",
    "                ticker = entry[2]               # Ticker\n",
    "                if ticker:  # Only add if there's a ticker\n",
    "                    mapping[cik] = [ticker]\n",
    "            print(f\"Loaded {len(mapping)} CIK-ticker mappings from SEC\")\n",
    "            return mapping\n",
    "        else:\n",
    "            print(f\"Error loading SEC data: Status code {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SEC data: {str(e)}\")\n",
    "    return {}\n",
    "\n",
    "# Load the CIK-ticker mapping once at startup\n",
    "print(\"Loading SEC ticker data...\")\n",
    "cik_ticker_cache = get_sec_tickers()\n",
    "processed_ciks = set(cik_ticker_cache.keys())\n",
    "\n",
    "def extract_company_name(content):\n",
    "    \"\"\"\n",
    "    Extracts the company name from the file content.\n",
    "    \"\"\"\n",
    "    match = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)$', content, re.MULTILINE)\n",
    "    return match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "def extract_company_id(content):\n",
    "    \"\"\"\n",
    "    Extracts the company identifier (CIK) from the file content.\n",
    "    \"\"\"\n",
    "    cik_pattern = r'CENTRAL INDEX KEY:\\s*(\\d{10})'\n",
    "    match = re.search(cik_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    alt_pattern = r'CIK=(\\d{10})'\n",
    "    match = re.search(alt_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_file_batch(file_path, cache):\n",
    "    \"\"\"Process a single file and return CIK and ticker info\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            cik = extract_company_id(content)\n",
    "            if cik:\n",
    "                # If CIK exists in our mapping, return it\n",
    "                if cik in cache:\n",
    "                    return cik, cache[cik]\n",
    "                processed_ciks.add(cik)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "    return None, None\n",
    "\n",
    "def create_cik_ticker_mapping(base_directory, output_file='cik_ticker_mapping.json', batch_size=1000):\n",
    "    \"\"\"\n",
    "    Creates a JSON file mapping CIK numbers to their ticker symbols in batches.\n",
    "    \"\"\"\n",
    "    # Create a new dictionary for storing only matches from your files\n",
    "    found_mappings = {}\n",
    "    \n",
    "    # Get list of all files\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt') and ('10-k' in file.lower() or '10-k405' in file.lower()) and not any(x in file.lower() for x in ['10-k/a', '10-k-a', '10-k405-a']):\n",
    "                files_list.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"Found {len(files_list)} 10-K files to process\")\n",
    "\n",
    "    # Process files in batches\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    for i in range(0, len(files_list), batch_size):\n",
    "        batch = files_list[i:i + batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1} of {(len(files_list) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_file = {executor.submit(process_file_batch, file_path, cik_ticker_cache): file_path \n",
    "                            for file_path in batch}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_file), total=len(batch), desc=\"Processing files\"):\n",
    "                cik, tickers = future.result()\n",
    "                if cik and tickers:  # Only add to found_mappings if both cik and tickers exist\n",
    "                    found_mappings[cik] = tickers\n",
    "        \n",
    "        # Save intermediate results after each batch\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(found_mappings, f, indent=4)  # Save only the found mappings\n",
    "        print(f\"Intermediate results saved. Current CIKs mapped: {len(found_mappings)}\")\n",
    "    \n",
    "    print(f\"\\nFinal CIK-Ticker mapping saved to {output_file}\")\n",
    "    print(f\"Total CIKs mapped: {len(found_mappings)}\")\n",
    "    return found_mappings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_cik_ticker_mapping(base_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "base_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data'\n",
    "\n",
    "def find_cik_in_files(target_cik, base_directory):\n",
    "    \"\"\"\n",
    "    Search for a specific CIK number using filenames.\n",
    "    \"\"\"\n",
    "    files_found = []\n",
    "    target_cik_no_zeros = str(int(target_cik))  # Remove leading zeros for matching\n",
    "    \n",
    "    # Walk through directory and check filenames\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if (file.lower().endswith('.txt') and \n",
    "                ('10-k' in file.lower() or '10-k405' in file.lower()) and \n",
    "                not any(x in file.lower() for x in ['10-k/a', '10-k-a', '10-k405-a'])):\n",
    "                \n",
    "                # Extract CIK from filename\n",
    "                match = re.search(r'edgar_data_(\\d+)_', file)\n",
    "                if match:\n",
    "                    file_cik = match.group(1)\n",
    "                    if file_cik == target_cik_no_zeros:\n",
    "                        files_found.append(os.path.join(root, file))\n",
    "    \n",
    "    # Print results\n",
    "    if files_found:\n",
    "        print(f\"\\nFound CIK {target_cik} in {len(files_found)} 10-K files:\")\n",
    "        for file in files_found:\n",
    "            print(f\"- {file}\")\n",
    "    else:\n",
    "        print(f\"\\nCIK {target_cik} not found in any 10-K files.\")\n",
    "    \n",
    "    return files_found\n",
    "\n",
    "# Search for the specific CIK\n",
    "find_cik_in_files(\"0000789019\", base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def check_file_cik(args):\n",
    "    \"\"\"Check if file's CIK is in our mapping and copy if it is\"\"\"\n",
    "    file_path, cik_mapping, output_dir = args\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            # Extract CIK using your existing patterns\n",
    "            cik_pattern = r'CENTRAL INDEX KEY:\\s*(\\d{10})'\n",
    "            alt_pattern = r'CIK=(\\d{10})'\n",
    "            \n",
    "            match = re.search(cik_pattern, content)\n",
    "            if not match:\n",
    "                match = re.search(alt_pattern, content)\n",
    "            \n",
    "            if match:\n",
    "                cik = match.group(1)\n",
    "                if cik in cik_mapping:\n",
    "                    # Create same directory structure in output_dir\n",
    "                    rel_path = os.path.relpath(file_path, base_directory)\n",
    "                    new_path = os.path.join(output_dir, rel_path)\n",
    "                    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "                    shutil.copy2(file_path, new_path)\n",
    "                    return new_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def copy_mapped_files(base_directory, mapping_file, output_directory):\n",
    "    \"\"\"Copy files with matching CIKs to new directory\"\"\"\n",
    "    \n",
    "    # Load CIK mapping\n",
    "    with open(mapping_file, 'r') as f:\n",
    "        cik_mapping = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(cik_mapping)} CIK-ticker mappings\")\n",
    "    \n",
    "    # Get list of all 10-K files\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt') and ('10-k' in file.lower() or '10-k405' in file.lower()) and not any(x in file.lower() for x in ['10-k/a', '10-k-a', '10-k405-a']):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"Found {len(all_files)} 10-K files to process\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Process files in parallel\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    copied_files = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        args = [(file_path, cik_mapping, output_directory) for file_path in all_files]\n",
    "        future_to_file = {executor.submit(check_file_cik, arg): arg[0] for arg in args}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Copying matched files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result:\n",
    "                    copied_files.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "\n",
    "    print(f\"\\nCopied {len(copied_files)} files to {output_directory}\")\n",
    "    return copied_files\n",
    "\n",
    "# Usage\n",
    "base_directory = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data'\n",
    "mapping_file = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\cik_ticker_mapping.json'\n",
    "output_directory = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Mapped_Files'\n",
    "\n",
    "copied_files = copy_mapped_files(base_directory, mapping_file, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Define the management patterns\n",
    "management_pattern = re.compile(r'ITEM\\s+7\\.?\\s+MANAGEMENT\\'?\\s*S?(.*?)ITEM\\s+[7A|7(a)|8]', re.IGNORECASE|re.DOTALL|re.MULTILINE)\n",
    "fallback_pattern = re.compile(r'MANAGEMENT\\'?\\s*S?\\s*DISCUSSION(.*?)(?:ITEM\\s+[7A|7(a)|8]|\\Z)', re.IGNORECASE|re.DOTALL|re.MULTILINE)\n",
    "\n",
    "# Define a pattern to check for other items\n",
    "other_items_pattern = re.compile(r'\\b(?:Item|ITEM)\\s+(?!7\\b)\\d+', re.IGNORECASE)\n",
    "\n",
    "def contains_other_items(mda_content):\n",
    "    \"\"\"\n",
    "    Checks if the MDA content contains mentions of items other than Item 7 more than two times.\n",
    "\n",
    "    Parameters:\n",
    "    - mda_content (str): The content of the MDA section.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if other items are mentioned more than twice, False otherwise.\n",
    "    \"\"\"\n",
    "    matches = other_items_pattern.findall(mda_content)\n",
    "    return len(matches) > 2\n",
    "\n",
    "def check_management_pattern(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        # Check for management pattern using findall\n",
    "        management_matches = management_pattern.findall(content)\n",
    "        \n",
    "        # Filter out matches with less than 500 words and check for other items\n",
    "        valid_matches = []\n",
    "        for match in management_matches:\n",
    "            word_count = len(match.split())\n",
    "            if word_count >= 500 and not contains_other_items(match):\n",
    "                valid_matches.append((match, word_count))\n",
    "        \n",
    "        # If no valid matches found, try fallback pattern\n",
    "        # if not valid_matches:\n",
    "        #     fallback_matches = fallback_pattern.findall(content)\n",
    "        #     for match in fallback_matches:\n",
    "        #         word_count = len(match.split())\n",
    "        #         if word_count >= 500 and not contains_other_items(match):\n",
    "        #             valid_matches.append((match, word_count))\n",
    "        \n",
    "        if not valid_matches:\n",
    "            return {\n",
    "                'file_path': file_path,\n",
    "                'has_management_pattern': False,\n",
    "                'longest_match': \"\",\n",
    "                'word_count': 0,\n",
    "                'has_other_items': False\n",
    "            }\n",
    "        \n",
    "        # Get the longest match\n",
    "        longest_match = max(valid_matches, key=lambda x: x[1])\n",
    "        \n",
    "        # Get 50 words before the longest match\n",
    "        words_before = ' '.join(content.split()[:content.index(longest_match[0])]).split()[-50:]\n",
    "        prefix = ' '.join(words_before)\n",
    "        \n",
    "        return {\n",
    "            'file_path': file_path,\n",
    "            'has_management_pattern': True,\n",
    "            'longest_match': prefix + ' ' + longest_match[0],\n",
    "            'word_count': longest_match[1],\n",
    "            'has_other_items': False\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    base_directory = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Mapped_Files'\n",
    "    output_file = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\longest_mdas.txt'\n",
    "    \n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt') and ('10-k' in file.lower() or '10-k405' in file.lower()) and not any(x in file.lower() for x in ['10-k/a', '10-k-a', '10-k405-a']):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(check_management_pattern, file_path): file_path for file_path in all_files}\n",
    "        \n",
    "        all_results = []\n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Processing files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result['has_management_pattern']:\n",
    "                    all_results.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "\n",
    "    # Write longest MDAs to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for result in all_results:\n",
    "            f.write(f\"File: {result['file_path']}\\n\")\n",
    "            f.write(f\"Word count: {result['word_count']}\\n\")\n",
    "            f.write(f\"Contains other items more than twice: No\\n\\n\")\n",
    "            f.write(result['longest_match'].strip())\n",
    "            f.write(\"\\n\\n---------------\\n\\n\")\n",
    "\n",
    "    print(f\"Total files processed: {len(all_files)}\")\n",
    "    print(f\"Files with valid management pattern: {len(all_results)}\")\n",
    "    print(f\"Longest MDAs saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\10-X_C_2001-2005\\2001\\QTR1\\20010201_10-K_edgar_data_775473_0001012870-01-000343.txt\"\n",
    "\n",
    "\n",
    "def check_management_pattern(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        # Check for management pattern using findall\n",
    "        management_matches = management_pattern.findall(content)\n",
    "        \n",
    "        # Filter out matches with less than 500 words and check for other items\n",
    "        valid_matches = []\n",
    "        for match in management_matches:\n",
    "            word_count = len(match.split())\n",
    "            # if word_count >= 0 and not contains_other_items(match):\n",
    "            #     valid_matches.append((match, word_count))\n",
    "            valid_matches.append(match)\n",
    "        \n",
    "        if not valid_matches:\n",
    "            return {\n",
    "                'file_path': file_path,\n",
    "                'has_management_pattern': False,\n",
    "                'longest_match': \"\",\n",
    "                'word_count': 0,\n",
    "                'has_other_items': False\n",
    "            }\n",
    "        \n",
    "        # Get the longest match\n",
    "        #longest_match = max(valid_matches, key=lambda x: x[1])\n",
    "        \n",
    "        # Get 50 words before the longest match\n",
    "        #words_before = ' '.join(content.split()[:content.index(longest_match[0])]).split()[-50:]\n",
    "        #prefix = ' '.join(words_before)\n",
    "        \n",
    "        return {\n",
    "            'file_path': file_path,\n",
    "            'has_management_pattern': True,\n",
    "            #'longest_match': prefix + ' ' + longest_match[0],\n",
    "            #'word_count': longest_match[1],\n",
    "            'matches': valid_matches,\n",
    "            'has_other_items': False\n",
    "        }\n",
    "\n",
    "# Check if the file exists before processing\n",
    "if os.path.exists(file_path):\n",
    "    # Call the check_management_pattern function for this single file\n",
    "    result = check_management_pattern(file_path)\n",
    "\n",
    "    # Print the result\n",
    "    if result['has_management_pattern']:\n",
    "        print(f\"File: {os.path.basename(result['file_path'])}\")\n",
    "        print(f\"Contains other items more than twice: {'Yes' if result['has_other_items'] else 'No'}\")\n",
    "        print(\"\\nExtracted MDA content:\")\n",
    "        pprint(result['matches'], width = 120)\n",
    "    else:\n",
    "        print(f\"No valid management pattern found in {os.path.basename(file_path)}\")\n",
    "else:\n",
    "    print(f\"Error: File not found - {file_path}\")\n",
    "    print(\"Please check the file path and ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\10-X_C_2001-2005\\2001\\QTR1\\20010117_10-K405-A_edgar_data_785786_0000950124-01-000209.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    print(f\"Successfully opened file: {os.path.basename(file_path)}\")\n",
    "    print(f\"File content: \", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_extracted_MDAs_2001-2020.csv')\n",
    "\n",
    "# Get unique company names\n",
    "unique_companies = df['company_name'].unique()\n",
    "\n",
    "# Print the list of unique company names\n",
    "print(\"List of unique company names:\")\n",
    "for company in unique_companies:\n",
    "    print(company)\n",
    "\n",
    "# Print the total count of unique companies\n",
    "print(f\"\\nTotal number of unique companies: {len(unique_companies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_extracted_MDAs_2001-2020.csv')\n",
    "\n",
    "# Get all unique CIK numbers\n",
    "cik_numbers = df['cik_number'].unique()\n",
    "cik_numbers = cik_numbers[0:1000]\n",
    "\n",
    "# Initialize counters\n",
    "total_companies = len(cik_numbers)\n",
    "companies_with_tickers = 0\n",
    "\n",
    "# Fetch ticker symbols for each CIK\n",
    "for cik in cik_numbers:\n",
    "    tickers = cik_to_ticker(str(cik))\n",
    "    if tickers:\n",
    "        companies_with_tickers += 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTotal companies: {total_companies}\")\n",
    "print(f\"Companies with tickers: {companies_with_tickers}\")\n",
    "print(f\"Companies without tickers: {total_companies - companies_with_tickers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the MDA contents of the first row from the CSV file and store them in a txt file\n",
    "import os\n",
    "\n",
    "# Define the input CSV file path\n",
    "input_csv_path = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_paired_mda_reports.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(input_csv_path):\n",
    "    # Read the first row of the CSV file\n",
    "    df = pd.read_csv(input_csv_path, nrows=2)\n",
    "    \n",
    "    if not df.empty and 'current_mda_content' in df.columns and 'next_mda_content' in df.columns:\n",
    "        # Get the MDA content from the first row\n",
    "        first_mda_content = df.iloc[1]['current_mda_content']\n",
    "        second_mda_content = df.iloc[1]['next_mda_content']\n",
    "\n",
    "        company_name = df.iloc[1]['company_name']\n",
    "        \n",
    "        # Define the output file path\n",
    "        output_file_path = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\mda_contents.txt'\n",
    "        \n",
    "        # Write the content to a txt file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            \n",
    "            file.write(f\"company_name: {company_name}\\n\")\n",
    "            file.write('Current MDA Content:\\n')\n",
    "            file.write(first_mda_content)\n",
    "            file.write('\\n\\n---------------------------\\n\\n')\n",
    "            file.write('Next MDA Content:\\n')\n",
    "            file.write(second_mda_content)\n",
    "\n",
    "        print(f\"First MDA content has been saved to: {output_file_path}\")\n",
    "    else:\n",
    "        print(\"Error: CSV file is empty or does not contain 'mda_content' column.\")\n",
    "else:\n",
    "    print(f\"Error: CSV file not found at {input_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDA Extraction Test 1 \n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import traceback\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the path for the log file\n",
    "log_directory = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output'\n",
    "log_file = os.path.join(log_directory, 'mda_extraction.log')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_file,\n",
    "    filemode='w'  # 'w' mode overwrites the file, use 'a' for append\n",
    ")\n",
    "\n",
    "# Add a StreamHandler for tqdm to work correctly\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.ERROR)  # Only show ERROR level logs in the console\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Define Regular Expressions\n",
    "# ================================\n",
    "\n",
    "\n",
    "#MDA Start Patterns\n",
    "ITEM7_PATTERNS = [\n",
    "\n",
    "    r'\\b(?:Item|ITEM)\\s*7\\b(?!A|\\.A)\\.?(?:\\s*[-–—])?\\s*(?:Management[\\'\\s]?s?|Managements?|Managment|Manag[ae]?ment[\\'\\s]?s?)\\b'\n",
    "]\n",
    "\n",
    "ITEM7_REGEX = re.compile('|'.join(ITEM7_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# MDA End Patterns\n",
    "ITEM7A_OR_8_PATTERNS = [\n",
    "    r'\\bItem\\s*7A\\.?\\s*',\n",
    "    r'\\bITEM\\s*7A\\.?\\s*',\n",
    "    r'\\bItem\\s*7A\\.?\\s*Quantitative\\s+and\\s+Qualitative\\s+Disclosures\\s+About\\s+Market\\s+Risk\\b',\n",
    "    r'\\bITEM\\s*7A\\.?\\s*Quantitative\\s+and\\s+Qualitative\\s+Disclosures\\s+About\\s+Market\\s+Risk\\b',\n",
    "    r'\\bItem\\s*8\\.?\\s*',\n",
    "    r'\\bITEM\\s*8\\.?\\s*',\n",
    "    r'\\bItem\\s*7\\.?\\s*Financial\\s+Statements\\b',\n",
    "    r'\\bITEM\\s*7\\.?\\s*Financial\\s+Statements\\b',\n",
    "]\n",
    "ITEM7A_OR_8_REGEX = re.compile('|'.join(ITEM7A_OR_8_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. Helper Functions\n",
    "# ================================\n",
    "\n",
    "\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch daily stock price data for a given ticker and date range.\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    data = stock.history(start=start_date, end=end_date)\n",
    "    return data['Close']\n",
    "\n",
    "def calculate_mdd(prices):\n",
    "    \"\"\"\n",
    "    Calculate Maximum Drawdown (MDD) for a given price series.\n",
    "    \"\"\"\n",
    "    peak = prices.iloc[0]\n",
    "    mdd = 0\n",
    "    for price in prices:\n",
    "        if price > peak:\n",
    "            peak = price\n",
    "        dd = (peak - price) / peak\n",
    "        if dd > mdd:\n",
    "            mdd = dd\n",
    "    return mdd\n",
    "\n",
    "def label_risk(mdds):\n",
    "    \"\"\"\n",
    "    Label companies as High Risk (1) or Normal Risk (0) based on MDD percentiles.\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(mdds, 80)\n",
    "    return [1 if mdd >= threshold else 0 for mdd in mdds]\n",
    "\n",
    "def prepare_risk_prediction_data(results):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for risk prediction.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for cik in set(r['cik_number'] for r in results):\n",
    "        company_reports = sorted([r for r in results if r['cik_number'] == cik], key=lambda x: x['filing_date'])\n",
    "        for i in range(1, len(company_reports)):\n",
    "            current_report = company_reports[i]\n",
    "            previous_report = company_reports[i-1]\n",
    "            \n",
    "            # Fetch stock data for the next year\n",
    "            start_date = current_report['filing_date']\n",
    "            end_date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "            \n",
    "            try:\n",
    "                stock_prices = get_stock_data(cik, start_date, end_date)\n",
    "                \n",
    "                if len(stock_prices) > 0:\n",
    "                    mdd = calculate_mdd(stock_prices)\n",
    "                    data.append({\n",
    "                        'cik': cik,\n",
    "                        'company_name': current_report['company_name'],\n",
    "                        'current_mda': current_report['mda_content'],\n",
    "                        'previous_mda': previous_report['mda_content'],\n",
    "                        'mdd': mdd,\n",
    "                        'filing_date': current_report['filing_date']\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error fetching stock data for CIK {cik}: {e}\")\n",
    "    \n",
    "    # Label risks\n",
    "    df = pd.DataFrame(data)\n",
    "    df['year'] = pd.to_datetime(df['filing_date']).dt.year\n",
    "    \n",
    "    def label_risk_by_year(group):\n",
    "        mdds = group['mdd'].values\n",
    "        return label_risk(mdds)\n",
    "    \n",
    "    df['risk_label'] = df.groupby('year').apply(label_risk_by_year).reset_index(drop=True)\n",
    "    \n",
    "    return df.to_dict('records')\n",
    "    \n",
    "    \n",
    "def contains_other_items(mda_content):\n",
    "    \"\"\"\n",
    "    Checks if the MDA content contains mentions of items other than Item 7.\n",
    "\n",
    "    Parameters:\n",
    "    - mda_content (str): The content of the MDA section.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if other items are mentioned, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # This pattern matches \"Item\" or \"ITEM\" followed by a number that's not 7\n",
    "    pattern = r'\\b(?:Item|ITEM)\\s+(?!7\\b)\\d+'\n",
    "    return bool(re.search(pattern, mda_content))\n",
    "\n",
    "def extract_company_name(content):\n",
    "    \"\"\"\n",
    "    Extracts the company name from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted company name or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)$', content, re.MULTILINE)\n",
    "    return match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "def extract_filing_date(content):\n",
    "    \"\"\"\n",
    "    Extracts the filing date from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted filing date in YYYY-MM-DD format or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'FILED AS OF DATE:\\s*(\\d{8})', content)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_company_id(content):\n",
    "    \"\"\"\n",
    "    Extracts the company identifier (CIK) from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The content of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: Extracted CIK or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Look for the CIK in the header of the file\n",
    "    cik_pattern = r'CENTRAL INDEX KEY:\\s*(\\d{10})'\n",
    "    match = re.search(cik_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # If not found in the standard location, try an alternative pattern\n",
    "    alt_pattern = r'CIK=(\\d{10})'\n",
    "    match = re.search(alt_pattern, content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def count_companies(content):\n",
    "    \"\"\"\n",
    "    Counts the number of unique companies in the SEC filing based on CIK numbers,\n",
    "    company names, and filer identifiers.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of unique companies detected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract CIK numbers using the existing helper function\n",
    "    cik_number = extract_company_id(content)\n",
    "    cik_numbers = set([cik_number]) if cik_number else set()\n",
    "\n",
    "    # Extract company names using the existing helper function\n",
    "    company_name = extract_company_name(content)\n",
    "    company_names = set([company_name]) if company_name != \"Unknown\" else set()\n",
    "\n",
    "    # Search for filer identifiers\n",
    "    filer_pattern = r'FILER:\\s*\\n\\s*COMPANY DATA:'\n",
    "    filer_count = len(re.findall(filer_pattern, content))\n",
    "\n",
    "    # Determine the number of unique companies\n",
    "    num_companies = max(len(cik_numbers), len(company_names), filer_count)\n",
    "\n",
    "    return num_companies\n",
    "\n",
    "# Filter through table of contents\n",
    "def is_table_of_contents(text, strict=True):\n",
    "    \"\"\"\n",
    "    Determines if the given text is likely to be a table of contents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to analyze.\n",
    "    - strict (bool): Whether to use strict or lenient criteria.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the text is likely a table of contents, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Checking if the text is a table of contents...\")\n",
    "    # Check if the text is short\n",
    "    if len(text.split()) < 200:\n",
    "        logging.info(\"Text is short, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for patterns typical in table of contents\n",
    "    toc_patterns = [\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\.\\s+.*\\d+\\s*$',  # Item followed by number and page number\n",
    "        r'\\bTable\\s+of\\s+Contents\\b',\n",
    "        r'\\b(Page|p\\.)\\s+\\d+\\b',\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\..*\\n.*\\d+\\s*$',  # Item title followed by page number on next line\n",
    "        r'^\\s*\\d+\\s*$',  # Standalone page numbers\n",
    "        r'\\bItem\\s+\\d+[A-Z]?.*\\d+\\s*\\b',  # Item followed by any text and a number (page number)\n",
    "        r'\\bItem\\s+\\d+[A-Z]?\\.?\\s+[^.]+?(?=\\s+Item|\\s+\\d+|$)'  # Item followed by title, ending at next Item or number\n",
    "    ]\n",
    "    \n",
    "    toc_regex = re.compile('|'.join(toc_patterns), re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Count matches\n",
    "    matches = toc_regex.findall(text)\n",
    "    if strict and len(matches) > 3:  # If more than 3 matches, likely a table of contents\n",
    "        logging.info(f\"Detected {len(matches)} matches in strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    elif not strict and len(matches) > 5:  # Less strict for longer sections\n",
    "        logging.info(f\"Detected {len(matches)} matches in less strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for consecutive item listings\n",
    "    items = re.findall(r'\\bItem\\s+\\d+[A-Z]?', text, re.IGNORECASE)\n",
    "    if len(items) > 2:  # More than 2 ITEM listings\n",
    "        # Check if these items appear close to each other\n",
    "        item_positions = [m.start() for m in re.finditer(r'\\bItem\\s+\\d+[A-Z]?', text, re.IGNORECASE)]\n",
    "        if len(item_positions) > 1:\n",
    "            avg_distance = sum(item_positions[i+1] - item_positions[i] for i in range(len(item_positions)-1)) / (len(item_positions)-1)\n",
    "            if strict and avg_distance < 200:  # If average distance between ITEMs is less than 200 characters\n",
    "                logging.info(\"High density of item listings detected, likely a table of contents.\")\n",
    "                return True\n",
    "            elif not strict and avg_distance < 100:  # Less strict for longer sections\n",
    "                logging.info(\"High density of item listings detected in less strict mode, likely a table of contents.\")\n",
    "                return True\n",
    "    \n",
    "    # Check for high density of item listings\n",
    "    words = text.split()\n",
    "    item_density = len(items) / len(words)\n",
    "    if strict and item_density > 0.01:  # If more than 1% of words are item listings, likely a table of contents\n",
    "        logging.info(\"Item density exceeds 1%, likely a table of contents.\")\n",
    "        return True\n",
    "    elif not strict and item_density > 0.02:  # Less strict for longer sections\n",
    "        logging.info(\"Item density exceeds 2% in less strict mode, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    # Check for short paragraphs (typical in TOC)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    short_paragraphs = [p for p in paragraphs if len(p.split()) < 20]\n",
    "    \n",
    "    if len(short_paragraphs) / len(paragraphs) > 0.5:  # If more than half of paragraphs are short, likely a TOC\n",
    "        logging.info(\"More than half of paragraphs are short, likely a table of contents.\")\n",
    "        return True\n",
    "    \n",
    "    logging.info(\"Text does not appear to be a table of contents.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def pair_reports(results):\n",
    "    \"\"\"\n",
    "    Pairs reports for companies based on their filing dates, allowing for delays and differences in publication dates.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): Extracted MDA sections with filenames and filing dates.\n",
    "\n",
    "    Returns:\n",
    "    - list of dict: Paired reports.\n",
    "    \"\"\"\n",
    "    # Group reports by company\n",
    "    company_reports = {}\n",
    "    for report in results:\n",
    "        cik = report['cik_number']\n",
    "        if cik not in company_reports:\n",
    "            company_reports[cik] = []\n",
    "        company_reports[cik].append(report)\n",
    "\n",
    "    paired_reports = []\n",
    "    for cik, reports in company_reports.items():\n",
    "        # Sort reports by filing date\n",
    "        sorted_reports = sorted(reports, key=lambda x: datetime.strptime(x['filing_date'], '%Y-%m-%d'))\n",
    "        \n",
    "        for i in range(len(sorted_reports) - 1):\n",
    "            current_report = sorted_reports[i]\n",
    "            next_report = sorted_reports[i + 1]\n",
    "            \n",
    "            current_date = datetime.strptime(current_report['filing_date'], '%Y-%m-%d')\n",
    "            next_date = datetime.strptime(next_report['filing_date'], '%Y-%m-%d')\n",
    "            \n",
    "            # Check if reports are within a reasonable time frame (e.g., 9-15 months apart)\n",
    "            if timedelta(days=270) <= next_date - current_date <= timedelta(days=450):\n",
    "                paired_reports.append({\n",
    "                    'current_filename': current_report['filename'],\n",
    "                    'next_filename': next_report['filename'],\n",
    "                    'company_name': current_report['company_name'],\n",
    "                    'cik_number': cik,\n",
    "                    'current_filing_date': current_report['filing_date'],\n",
    "                    'next_filing_date': next_report['filing_date'],\n",
    "                    'current_mda_content': current_report['mda_content'],\n",
    "                    'next_mda_content': next_report['mda_content'],\n",
    "                    'time_difference': (next_date - current_date).days\n",
    "                })\n",
    "\n",
    "    return paired_reports\n",
    "\n",
    "def pair_and_save_reports(results, output_directory):\n",
    "    \"\"\"\n",
    "    Pairs reports and saves them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - results (list of dict): Extracted MDA sections with filenames and filing dates.\n",
    "    - output_directory (str): Directory to save the paired reports CSV.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    paired_reports_csv = os.path.join(output_directory, 'paired_mda_reports.csv')\n",
    "    \n",
    "    # Use the new pair_reports function that doesn't rely on fiscal_calendar_df\n",
    "    paired_reports = pair_reports(results)\n",
    "    \n",
    "    if paired_reports:\n",
    "        fieldnames = ['current_filename', 'next_filename', 'company_name', 'cik_number', \n",
    "                      'current_filing_date', 'next_filing_date', 'current_mda_content', \n",
    "                      'next_mda_content', 'time_difference']\n",
    "        \n",
    "        with open(paired_reports_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in paired_reports:\n",
    "                writer.writerow(row)\n",
    "        logging.info(f\"INFO: Paired reports saved to {paired_reports_csv}\")\n",
    "    else:\n",
    "        logging.warning(\"WARNING: No paired reports found.\")\n",
    "\n",
    "# ================================\n",
    "# 2. MDA Text Extraction\n",
    "# ================================\n",
    "\n",
    "\n",
    "def extract_mda_section(text):\n",
    "    \"\"\"\n",
    "    Extracts the Management's Discussion and Analysis (MDA) section from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary containing the extracted MDA section and metadata, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Normalizing text for extraction...\")\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    item7_matches = list(ITEM7_REGEX.finditer(normalized_text))\n",
    "    logging.info(f\"Found {len(item7_matches)} potential Item 7 matches.\")\n",
    "    \n",
    "    if not item7_matches:\n",
    "        logging.info(\"No Item 7 matches found.\")\n",
    "        return None\n",
    "    \n",
    "    for start_match in item7_matches:\n",
    "        start_index = start_match.start()\n",
    "        start_pattern = start_match.group()\n",
    "        logging.info(f\"Found potential MDA start at index {start_index}: '{start_pattern}'\")\n",
    "        \n",
    "        if 'item 6' in start_pattern.lower() and 'management' not in start_pattern.lower() and 'md&a' not in start_pattern.lower():\n",
    "            logging.info(\"Skipping Item 6 as it is not explicitly an MDA.\")\n",
    "            continue\n",
    "        \n",
    "        # Find end of MDA\n",
    "        end_match = ITEM7A_OR_8_REGEX.search(normalized_text[start_index + 100:])\n",
    "        if end_match:\n",
    "            end_index = start_index + 100 + end_match.start()\n",
    "            logging.info(f\"Found potential MDA end at index {end_index}.\")\n",
    "        else:\n",
    "            end_index = len(normalized_text)\n",
    "            logging.info(\"No end pattern found, using end of document.\")\n",
    "        \n",
    "        mda_text = normalized_text[start_index:end_index]\n",
    "        \n",
    "        # Check if the entire section is a table of contents\n",
    "        if is_table_of_contents(mda_text):\n",
    "            logging.info(\"This section appears to be a table of contents. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # If the beginning looks like a table of contents, try to find the real start of the MDA\n",
    "        first_1000_words = ' '.join(mda_text.split()[:1000])\n",
    "        if is_table_of_contents(first_1000_words):\n",
    "            logging.info(\"The beginning looks like a table of contents. Searching for the real MDA start.\")\n",
    "            real_start_match = re.search(r'(Management\\'s\\s+Discussion\\s+and\\s+Analysis|MD&A).{0,500}?(?=\\n\\n)', mda_text, re.IGNORECASE | re.DOTALL)\n",
    "            if real_start_match:\n",
    "                start_index += real_start_match.start()\n",
    "                mda_text = mda_text[real_start_match.start():]\n",
    "                logging.info(f\"Found real MDA start at index {start_index}.\")\n",
    "            else:\n",
    "                logging.info(\"Couldn't find the real MDA start. Skipping this occurrence.\")\n",
    "                continue\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(mda_text.split())\n",
    "        logging.info(f\"Extracted MDA section with word count: {word_count}.\")\n",
    "        if 500 <= word_count:\n",
    "            return {\n",
    "                'mda_content': mda_text,\n",
    "                'start_index': start_index,\n",
    "                'end_index': end_index,\n",
    "                'start_pattern': start_pattern,\n",
    "                'end_pattern': end_match.group() if end_match else \"End of document\",\n",
    "                'word_count': word_count\n",
    "            }\n",
    "    \n",
    "    logging.info(\"No suitable MDA section found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Method to process each file \n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single 10-K file to extract the MDA section and related metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the 10-K file.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary containing the extracted MDA section and metadata, or None if extraction fails.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        num_companies = count_companies(content)\n",
    "\n",
    "        if num_companies != 1:\n",
    "            logging.info(f\"Skipping file {file_path}: Detected {num_companies} companies.\")\n",
    "            return None \n",
    "\n",
    "        cik_number = extract_company_id(content)\n",
    "        company_name = extract_company_name(content)\n",
    "        filing_date = extract_filing_date(content)\n",
    "\n",
    "        result = extract_mda_section(content)\n",
    "\n",
    "        if result:\n",
    "            result.update({\n",
    "                'cik_number': cik_number,\n",
    "                'company_name': company_name,\n",
    "                'filing_date': filing_date,\n",
    "                'filename': os.path.basename(file_path)\n",
    "            })\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "        \n",
    "# ================================\n",
    "# 3. Main Processing Function\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process 10-K files, extract MDA sections, and generate paired reports.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing filtered results and paired reports.\n",
    "    \"\"\"\n",
    "\n",
    "    base_directory = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data'\n",
    "    output_directory = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    all_results = []\n",
    "    all_files = []\n",
    "\n",
    "    # Process specified periods\n",
    "    for period in ['10-X_C_2001-2005', '10-X_C_2006-2010', '10-X_C_2011-2015', '10-X_C_2016-2020']:\n",
    "        period_path = os.path.join(base_directory, period)\n",
    "        if os.path.isdir(period_path):\n",
    "            for year in os.listdir(period_path):\n",
    "                year_path = os.path.join(period_path, year)\n",
    "                if os.path.isdir(year_path) and year.isdigit():\n",
    "                    for quarter in ['QTR1', 'QTR2', 'QTR3', 'QTR4']:\n",
    "                        quarter_path = os.path.join(year_path, quarter)\n",
    "                        if os.path.isdir(quarter_path):\n",
    "                            logging.info(f\"Processing {period} - {year} {quarter}\")\n",
    "                            quarter_files = [\n",
    "                                os.path.join(quarter_path, f)\n",
    "                                for f in os.listdir(quarter_path)\n",
    "                                if f.lower().endswith('.txt') and '10-k' in f.lower() and not any(x in f.lower() for x in ['10-k/a', '10-k-a'])\n",
    "                            ]\n",
    "                            all_files.extend(quarter_files)\n",
    "\n",
    "    # Process files concurrently\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file_path): file_path for file_path in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Processing files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "\n",
    "    # Filter out MDAs with mentions of items other than Item 7\n",
    "    filtered_results = [result for result in all_results if not contains_other_items(result['mda_content'])]\n",
    "    \n",
    "    logging.info(f\"Total MDAs extracted: {len(all_results)}\")\n",
    "    logging.info(f\"MDAs with only Item 7 mentioned: {len(filtered_results)}\")\n",
    "\n",
    "    # Save all results to a single CSV (including those with other items mentioned)\n",
    "    output_csv_file = os.path.join(output_directory, 'all_extracted_MDAs2006-2010.csv')\n",
    "    fieldnames = ['filename', 'company_name', 'cik_number', 'filing_date', 'mda_content', 'start_index', 'end_index', 'end_pattern', 'start_pattern', 'word_count']\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_results:\n",
    "            writer.writerow(row)\n",
    "    logging.info(f\"INFO: All extracted MDA sections and metadata saved to {output_csv_file}\")\n",
    "\n",
    "    # Pair reports using filtered results\n",
    "    paired_reports = pair_reports(filtered_results)\n",
    "\n",
    "    # Save paired reports\n",
    "    paired_reports_csv = os.path.join(output_directory, 'all_paired_mda_reports.csv')\n",
    "    if paired_reports:\n",
    "        paired_fieldnames = ['current_filename', 'next_filename', 'company_name', 'cik_number', \n",
    "                             'current_filing_date', 'next_filing_date', 'current_mda_content', \n",
    "                             'next_mda_content', 'time_difference']\n",
    "        with open(paired_reports_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=paired_fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in paired_reports:\n",
    "                writer.writerow(row)\n",
    "        logging.info(f\"INFO: All paired reports saved to {paired_reports_csv}\")\n",
    "    else:\n",
    "        logging.warning(\"WARNING: No paired reports found.\")\n",
    "\n",
    "    print(f\"Total files processed: {len(all_files)}\")\n",
    "    print(f\"Number of files with MDA sections: {len(all_results)}\")\n",
    "    print(f\"Number of MDAs with only Item 7 mentioned: {len(filtered_results)}\")\n",
    "    print(f\"Number of paired reports: {len(paired_reports)}\")\n",
    "\n",
    "    return filtered_results, paired_reports\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filtered_results, paired_reports = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Load the paired reports CSV\n",
    "paired_reports_df = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_paired_mda_reports.csv')\n",
    "output_file = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\pairedMDA_row1_preview.txt'\n",
    "\n",
    "# Check if there are any rows in the DataFrame\n",
    "if not paired_reports_df.empty:\n",
    "    # Get the first row\n",
    "    first_pair = paired_reports_df.iloc[1028]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Print the filenames and MDA contents\n",
    "        f.write(f\"Current filename: {first_pair['current_filename']}\")\n",
    "        f.write(f\"Length of MDA in words: {len(first_pair['current_mda_content'].split())}\")\n",
    "        f.write(f\"\\nCurrent MDA content: {first_pair['current_mda_content']}\")\n",
    "        f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")  # Separator\n",
    "        f.write(f\"Next filename: {first_pair['next_filename']}\")\n",
    "        f.write(f\"Length of MDA in words: {len(first_pair['next_mda_content'].split())}\")\n",
    "        f.write(f\"\\nNext MDA content: {first_pair['next_mda_content']}\")\n",
    "\n",
    "    print(\"File saved successfully.\")\n",
    "\n",
    "else:\n",
    "    print(\"No paired reports found in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's check for occurrences of \"ITEM 7A\" in the 'mda_content' to identify any that might belong to Item 7A instead of Item 7.\n",
    "data = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_extracted_MDAs2006-2010.csv')\n",
    "\n",
    "item_7a_instances = data[data['mda_content'].str.contains(\"ITEM 7A|Item 7A\", case=False, na=False)]\n",
    "\n",
    "# Display the rows where Item 7A is found\n",
    "print(item_7a_instances[['filename', 'mda_content']].head())\n",
    "# Get the count of rows with Item 7A mentioned\n",
    "item_7a_count = len(item_7a_instances)\n",
    "\n",
    "print(f\"Number of rows with Item 7A: {item_7a_count}\")\n",
    "\n",
    "# Print the first 50 words of each MDA content\n",
    "for index, row in item_7a_instances.iterrows():\n",
    "    filename = row['filename']\n",
    "    content = row['mda_content']\n",
    "    \n",
    "    # Split the content into words and take the first 50\n",
    "    words = content.split()[:50]\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    preview = ' '.join(words)\n",
    "    \n",
    "    print(f\"\\nContent from {filename}:\\n\")\n",
    "    print(preview)\n",
    "    print(\"\\n\" + \"-\" * 50)  # Print a line of 50 dashes as a separator\n",
    "\n",
    "\"\"\" \n",
    "                                                  filename  \\\n",
    "39   20110112_10-K_edgar_data_805305_0000950123-11-...   \n",
    "82   20110114_10-K_edgar_data_1156884_0001079974-11...   \n",
    "109  20110121_10-K_edgar_data_20740_0001144204-11-0...   \n",
    "110  20110120_10-K_edgar_data_39368_0001193125-11-0...   \n",
    "115  20110125_10-K_edgar_data_1090061_0001193125-11...   \n",
    "\n",
    "                                           mda_content  \n",
    "39   Item 7A. QUANTITATIVE AND QUALITATIVE DISCLOSU...  \n",
    "82   Item 7a. Quantitative And Qualitative Disclosu...  \n",
    "109  Item 7A. Quantitative and Qualitative Disclosu...  \n",
    "110  Item 7A. Quantitative and Qualitative Disclosu...  \n",
    "115  Item 7A. Quantitative and Qualitative Disclosu...  \n",
    "Number of rows with Item 7A: 878\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "                                             filename  \\\n",
    "513  20110223_10-K_edgar_data_1289790_0001188112-11...   \n",
    "688  20110225_10-K_edgar_data_1038357_0001193125-11...   \n",
    "706  20110225_10-K_edgar_data_1090012_0000950123-11...   \n",
    "716  20110225_10-K_edgar_data_1115836_0001104659-11...   \n",
    "763  20110225_10-K_edgar_data_1407463_0001407463-11...   \n",
    "\n",
    "                                           mda_content  \n",
    "513  Item 7, and Quantitative and Qualitative Discl...  \n",
    "688  Item 7. Management's Discussion and Analysis o...  \n",
    "706  Item 7. Management s Discussion and Analysis o...  \n",
    "716  Item 7 Management s Discussion and Analysis of...  \n",
    "763  Management's Discussion and Analysis of Financ...  \n",
    "Number of rows with Item 7A: 30\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_extracted_MDAs2006-2010.csv')\n",
    "\n",
    "# Define the output file path\n",
    "output_file = 'C:/Users/abbra/Documents/Research/Koval Paper/Data/Output/mda_previews.txt'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for index, row in data.iterrows():\n",
    "        filename = row['filename']\n",
    "        content = row['mda_content']\n",
    "        \n",
    "        # Split the content into words and take the first 100\n",
    "        words = content.split()[:100]\n",
    "        \n",
    "        # Join the words back into a string\n",
    "        preview = ' '.join(words)\n",
    "        \n",
    "        # Write to the file instead of printing\n",
    "        f.write(f\"\\nContent from {filename}:\\n\\n\")\n",
    "        f.write(preview)\n",
    "        f.write(\"\\n\\n\" + \"-\" * 50 + \"\\n\")  # Write a line of 50 dashes as a separator\n",
    "\n",
    "print(f\"Content previews have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import statistics\n",
    "\n",
    "# Path to the item_7_only CSV file\n",
    "item_7_only_csv_path = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_paired_mda_reports.csv'\n",
    "\n",
    "# List to store word counts of mda_content\n",
    "mda_word_counts = []\n",
    "\n",
    "try:\n",
    "    with open(item_7_only_csv_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            word_count1 = len(row['current_mda_content'].split())\n",
    "            word_count2 = len(row['next_mda_content'].split())\n",
    "            mda_word_counts.append(word_count1)\n",
    "            mda_word_counts.append(word_count2)\n",
    "\n",
    "    if mda_word_counts:\n",
    "        average_word_count = statistics.mean(mda_word_counts)\n",
    "        print(f\"Average word count of mda_content in Item 7 only MDAs: {average_word_count:.2f} words\")\n",
    "    else:\n",
    "        print(\"No Item 7 only MDAs found in the file.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_path = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_paired_mda_reports.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get the word counts of current_mda_content for rows 115-125\n",
    "word_counts = df.loc[114:124, 'current_mda_content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Word counts of current_mda_content for rows 115-125:\")\n",
    "for index, count in word_counts.items():\n",
    "    print(f\"Row {index + 1}: {count} words\")\n",
    "\n",
    "# Calculate and print the average word count\n",
    "average_word_count = word_counts.mean()\n",
    "print(f\"\\nAverage word count: {average_word_count:.2f} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_path = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\all_paired_mda_reports.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get the mda content of row 120 (index 119 in zero-based indexing)\n",
    "mda_content = df.loc[119, 'current_mda_content']\n",
    "\n",
    "# Define the destination directory (Downloads folder)\n",
    "destination_directory = 'C:/Users/abbra/Documents/Research/Koval Paper/Data/Output/'\n",
    "\n",
    "# Construct the full destination file path\n",
    "destination_file = os.path.join(destination_directory, 'row_120_mda_content.txt')\n",
    "\n",
    "# Write the mda content to a txt file\n",
    "try:\n",
    "    with open(destination_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(mda_content)\n",
    "    print(f\"MDA content from row 120 has been saved to {destination_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the source file path\n",
    "source_file = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\10-X_C_2001-2005\\2001\\QTR3\\20010918_10-K_edgar_data_789019_0001032210-01-501099.txt'\n",
    "\n",
    "# Define the destination directory (Downloads folder)\n",
    "# Adjust the path according to your username\n",
    "destination_directory = r'C:\\Users\\abbra\\Downloads'\n",
    "\n",
    "# Construct the full destination file path using the original file name\n",
    "destination_file = os.path.join(destination_directory, os.path.basename(source_file))\n",
    "\n",
    "# Copy the file\n",
    "try:\n",
    "    shutil.copy(source_file, destination_file)\n",
    "    print(f\"File copied successfully to {destination_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while copying the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "import sys\n",
    "\n",
    "# Try to set the field size limit to a large value\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int/10)\n",
    "\n",
    "# Define the source file path\n",
    "source_file = r'C:\\Users\\abbra\\Documents\\Research\\Koval Paper\\Data\\Output\\paired_mda_reports.csv'\n",
    "\n",
    "# Define the destination directory (Downloads folder)\n",
    "destination_directory = r'C:\\Users\\abbra\\Downloads'\n",
    "\n",
    "# Construct the full destination file path\n",
    "destination_file = os.path.join(destination_directory, 'testing_eighth.csv')\n",
    "\n",
    "try:\n",
    "    # Read the source CSV file\n",
    "    with open(source_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        all_rows = list(reader)\n",
    "    \n",
    "    # Calculate one eighth of the rows (rounding up)\n",
    "    eighth_point = ceil(len(all_rows) / 12)\n",
    "    \n",
    "    # Write the first eighth of the rows to the new CSV file\n",
    "    with open(destination_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(all_rows[:eighth_point])\n",
    "    \n",
    "    print(f\"First eighth of the file copied successfully to {destination_file}\")\n",
    "    print(f\"Number of rows in original file: {len(all_rows)}\")\n",
    "    print(f\"Number of rows in new file: {eighth_point}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while processing the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting files...\n",
      "Total number of files to process: 54551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 54551/54551 [03:14<00:00, 280.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total files with multiple companies: 2020\n",
      "\n",
      "10 example files with multiple companies:\n",
      "1. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K405_edgar_data_46207_0000898430-01-500022.txt (Number of companies: 2)\n",
      "2. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K405_edgar_data_354707_0000898430-01-500022.txt (Number of companies: 2)\n",
      "3. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010322_10-K_edgar_data_1067701_0000950130-01-500286.txt (Number of companies: 2)\n",
      "4. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010322_10-K_edgar_data_1047166_0000950130-01-500286.txt (Number of companies: 2)\n",
      "5. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K_edgar_data_899045_0000950134-01-002534.txt (Number of companies: 2)\n",
      "6. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K_edgar_data_811156_0000950124-01-001548.txt (Number of companies: 3)\n",
      "7. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K_edgar_data_76063_0000950124-01-001548.txt (Number of companies: 3)\n",
      "8. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K_edgar_data_201533_0000950124-01-001548.txt (Number of companies: 3)\n",
      "9. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K_edgar_data_1090425_0000950134-01-002534.txt (Number of companies: 2)\n",
      "10. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010316_10-K_edgar_data_7323_0000065984-01-000070.txt (Number of companies: 7)\n",
      "\n",
      "Checking if companies from multiple-company files are in paired_mda_reports.csv...\n",
      "\n",
      "Total multiple-company files: 2020\n",
      "Files found in paired_mda_reports: 1466\n",
      "Files not found in paired_mda_reports: 554\n",
      "\n",
      "Example files found in paired_mda_reports:\n",
      "\n",
      "1. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K405_edgar_data_46207_0000898430-01-500022.txt\n",
      "   CIK: 354707\n",
      "   Number of companies in file: 2\n",
      "\n",
      "2. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K405_edgar_data_354707_0000898430-01-500022.txt\n",
      "   CIK: 354707\n",
      "   Number of companies in file: 2\n",
      "\n",
      "3. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010322_10-K_edgar_data_1067701_0000950130-01-500286.txt\n",
      "   CIK: 1067701\n",
      "   Number of companies in file: 2\n",
      "\n",
      "4. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010322_10-K_edgar_data_1047166_0000950130-01-500286.txt\n",
      "   CIK: 1067701\n",
      "   Number of companies in file: 2\n",
      "\n",
      "5. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010323_10-K_edgar_data_899045_0000950134-01-002534.txt\n",
      "   CIK: 1090425\n",
      "   Number of companies in file: 2\n",
      "\n",
      "Example files not found in paired_mda_reports:\n",
      "\n",
      "1. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010316_10-K_edgar_data_7323_0000065984-01-000070.txt\n",
      "   CIK: 65984\n",
      "   Number of companies in file: 7\n",
      "\n",
      "2. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010316_10-K_edgar_data_71508_0000065984-01-000070.txt\n",
      "   CIK: 65984\n",
      "   Number of companies in file: 7\n",
      "\n",
      "3. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010316_10-K_edgar_data_66901_0000065984-01-000070.txt\n",
      "   CIK: 65984\n",
      "   Number of companies in file: 7\n",
      "\n",
      "4. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010316_10-K_edgar_data_65984_0000065984-01-000070.txt\n",
      "   CIK: 65984\n",
      "   Number of companies in file: 7\n",
      "\n",
      "5. C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files\\10-X_C_2001-2005\\2001\\QTR1\\20010316_10-K_edgar_data_60527_0000065984-01-000070.txt\n",
      "   CIK: 65984\n",
      "   Number of companies in file: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def extract_company_id(content):\n",
    "    \"\"\"\n",
    "    Extracts the company CIK number from the file content.\n",
    "    \n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The extracted CIK number or None if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'CENTRAL INDEX KEY:\\s*(\\d+)', content, re.MULTILINE)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def extract_company_name(content):\n",
    "    \"\"\"\n",
    "    Extracts the company name from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted company name or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)$', content, re.MULTILINE)\n",
    "    return match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "def check_multiple_companies(file_path):\n",
    "    \"\"\"\n",
    "    Checks if a file contains multiple companies.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the 10-K file\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing file path and whether it has multiple companies\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Extract CIK numbers using the existing helper function\n",
    "        cik_number = extract_company_id(content)\n",
    "        cik_numbers = set([cik_number]) if cik_number else set()\n",
    "\n",
    "        # Extract company names using the helper function\n",
    "        company_name = extract_company_name(content)\n",
    "        company_names = set([company_name]) if company_name != \"Unknown\" else set()\n",
    "\n",
    "        # Search for filer identifiers\n",
    "        filer_pattern = r'FILER:\\s*\\n\\s*COMPANY DATA:'\n",
    "        filer_count = len(re.findall(filer_pattern, content))\n",
    "\n",
    "        # Determine if there are multiple companies\n",
    "        num_companies = max(len(cik_numbers), len(company_names), filer_count)\n",
    "        \n",
    "        return {\n",
    "            'file_path': file_path,\n",
    "            'has_multiple_companies': num_companies > 1,\n",
    "            'num_companies': num_companies\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {e}\")\n",
    "        return {\n",
    "            'file_path': file_path,\n",
    "            'has_multiple_companies': False,\n",
    "            'num_companies': 0\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    base_directory = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Mapped_Files'\n",
    "    \n",
    "    # First count total number of files\n",
    "    all_files = []\n",
    "    total_files = 0\n",
    "    print(\"Counting files...\")\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt') and ('10-k' in file.lower() or '10-k405' in file.lower()) and not any(x in file.lower() for x in ['10-k/a', '10-k-a', '10-k405-a']):\n",
    "                total_files += 1\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Total number of files to process: {total_files}\")\n",
    "\n",
    "    max_workers = min(32, os.cpu_count() + 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(check_multiple_companies, file_path): file_path for file_path in all_files}\n",
    "        \n",
    "        multiple_company_files = []\n",
    "        for future in tqdm(as_completed(future_to_file), total=len(all_files), desc=\"Processing files\"):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                if result['has_multiple_companies']:\n",
    "                    multiple_company_files.append(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"ERROR: {file_path} generated an exception: {exc}\")\n",
    "    \n",
    "    print(f\"\\nTotal files with multiple companies: {len(multiple_company_files)}\")\n",
    "    print(\"\\n10 example files with multiple companies:\")\n",
    "    for i, file_info in enumerate(multiple_company_files[:10], 1):\n",
    "        print(f\"{i}. {file_info['file_path']} (Number of companies: {file_info['num_companies']})\")\n",
    "    \n",
    "    return multiple_company_files  # Return the list\n",
    "\n",
    "multiple_company_files = main()\n",
    "\n",
    "# Load the paired MDA reports file\n",
    "paired_mda_df = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\paired_mda_reports_CLEANED.csv')\n",
    "\n",
    "# Extract unique CIKs from paired_mda_reports\n",
    "paired_ciks = set(paired_mda_df['cik_number'].unique())\n",
    "\n",
    "\n",
    "print(\"\\nChecking if companies from multiple-company files are in paired_mda_reports.csv...\")\n",
    "\n",
    "# Create lists to store files where companies are found in paired_mda_reports\n",
    "found_companies = []\n",
    "missing_companies = []\n",
    "\n",
    "# Add tqdm for the second processing loop\n",
    "for file_info in tqdm(multiple_company_files, desc=\"Checking paired reports\"):\n",
    "    file_path = file_info['file_path']\n",
    "    \n",
    "    # Extract CIK from the file content\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            cik = extract_company_id(content)\n",
    "            \n",
    "            if cik:\n",
    "                cik = int(cik)  # Convert to integer to match format in paired_mda_reports\n",
    "                if cik in paired_ciks:\n",
    "                    found_companies.append({\n",
    "                        'file_path': file_path,\n",
    "                        'cik': cik,\n",
    "                        'num_companies': file_info['num_companies']\n",
    "                    })\n",
    "                else:\n",
    "                    missing_companies.append({\n",
    "                        'file_path': file_path,\n",
    "                        'cik': cik,\n",
    "                        'num_companies': file_info['num_companies']\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Calculate percentage of paired reports affected\n",
    "total_paired_reports = len(paired_mda_df)\n",
    "percent_affected = (len(found_companies) / total_paired_reports) * 100\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Total paired reports in CSV: {total_paired_reports}\")\n",
    "print(f\"Number of multiple-company files found in paired reports: {len(found_companies)}\")\n",
    "print(f\"Percentage of paired reports affected: {percent_affected:.2f}%\")\n",
    "\n",
    "# Print some examples of found companies\n",
    "print(\"\\nExample files found in paired_mda_reports:\")\n",
    "for i, file_info in enumerate(found_companies[:5], 1):\n",
    "    print(f\"\\n{i}. {file_info['file_path']}\")\n",
    "    print(f\"   CIK: {file_info['cik']}\")\n",
    "    print(f\"   Number of companies in file: {file_info['num_companies']}\")\n",
    "\n",
    "# Print some examples of missing companies\n",
    "print(\"\\nExample files not found in paired_mda_reports:\")\n",
    "for i, file_info in enumerate(missing_companies[:5], 1):\n",
    "    print(f\"\\n{i}. {file_info['file_path']}\")\n",
    "    print(f\"   CIK: {file_info['cik']}\")\n",
    "    print(f\"   Number of companies in file: {file_info['num_companies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive Multiple Companies Analysis:\n",
      "----------------------------------------\n",
      "Total paired rows: 27508\n",
      "Pairs with multiple companies: 654\n",
      "Percentage of pairs with multiple companies: 2.38%\n",
      "\n",
      "Distribution by Year:\n",
      "--------------------\n",
      "Year 2001: 25 pairs (3.41%)\n",
      "Year 2002: 22 pairs (2.81%)\n",
      "Year 2003: 23 pairs (2.81%)\n",
      "Year 2004: 22 pairs (2.55%)\n",
      "Year 2005: 25 pairs (2.75%)\n",
      "Year 2006: 27 pairs (2.75%)\n",
      "Year 2007: 35 pairs (3.28%)\n",
      "Year 2008: 30 pairs (2.51%)\n",
      "Year 2009: 33 pairs (2.25%)\n",
      "Year 2010: 33 pairs (2.16%)\n",
      "Year 2011: 37 pairs (2.26%)\n",
      "Year 2012: 36 pairs (2.15%)\n",
      "Year 2013: 30 pairs (1.71%)\n",
      "Year 2014: 41 pairs (2.26%)\n",
      "Year 2015: 44 pairs (2.32%)\n",
      "Year 2016: 45 pairs (2.24%)\n",
      "Year 2017: 49 pairs (2.36%)\n",
      "Year 2018: 51 pairs (2.36%)\n",
      "Year 2019: 46 pairs (2.15%)\n",
      "\n",
      "Cross-Reference with Original Analysis:\n",
      "--------------------------------------\n",
      "Original individual files count: 1,798\n",
      "Current analysis:\n",
      "- Unique CIKs with multiple companies: 85\n",
      "- Total pairs with multiple companies: 654\n",
      "- Estimated individual files: 1308\n",
      "\n",
      "Updated CSV saved to: C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\paired_mda_reports_CLEANEDV4.csv\n",
      "\n",
      "Sample of pairs with multiple companies:\n",
      "\n",
      "CIK: 924901\n",
      "Number of Companies: 2\n",
      "Current Filing Date: 2005-03-03\n",
      "Next Filing Date: 2006-02-23\n",
      "\n",
      "CIK: 6201\n",
      "Number of Companies: 2\n",
      "Current Filing Date: 2014-02-28\n",
      "Next Filing Date: 2015-02-25\n",
      "\n",
      "CIK: 30697\n",
      "Number of Companies: 2\n",
      "Current Filing Date: 2011-03-03\n",
      "Next Filing Date: 2012-03-01\n",
      "\n",
      "CIK: 910108\n",
      "Number of Companies: 2\n",
      "Current Filing Date: 2005-03-16\n",
      "Next Filing Date: 2006-03-14\n",
      "\n",
      "CIK: 827052\n",
      "Number of Companies: 2\n",
      "Current Filing Date: 2011-02-28\n",
      "Next Filing Date: 2012-02-29\n"
     ]
    }
   ],
   "source": [
    "# Create sets from found_companies\n",
    "multiple_company_ciks = set(file_info['cik'] for file_info in found_companies)\n",
    "cik_to_num_companies = {file_info['cik']: file_info['num_companies'] for file_info in found_companies}\n",
    "\n",
    "# Add columns to DataFrame\n",
    "paired_mda_df['has_multiple_companies'] = paired_mda_df['cik_number'].isin(multiple_company_ciks)\n",
    "paired_mda_df['num_companies'] = paired_mda_df['cik_number'].map(lambda x: cik_to_num_companies.get(x, 1))\n",
    "\n",
    "# Print comprehensive analysis\n",
    "print(\"\\nComprehensive Multiple Companies Analysis:\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"Total paired rows: {len(paired_mda_df)}\")\n",
    "print(f\"Pairs with multiple companies: {paired_mda_df['has_multiple_companies'].sum()}\")\n",
    "print(f\"Percentage of pairs with multiple companies: {(paired_mda_df['has_multiple_companies'].sum() / len(paired_mda_df)) * 100:.2f}%\")\n",
    "\n",
    "# Distribution by year\n",
    "print(\"\\nDistribution by Year:\")\n",
    "print(\"--------------------\")\n",
    "yearly_counts = paired_mda_df[paired_mda_df['has_multiple_companies']].groupby(\n",
    "    pd.to_datetime(paired_mda_df['current_filing_date']).dt.year\n",
    ").size()\n",
    "\n",
    "for year, count in yearly_counts.items():\n",
    "    total_in_year = len(paired_mda_df[pd.to_datetime(paired_mda_df['current_filing_date']).dt.year == year])\n",
    "    percentage = (count / total_in_year) * 100\n",
    "    print(f\"Year {year}: {count} pairs ({percentage:.2f}%)\")\n",
    "\n",
    "# Cross-reference with original analysis\n",
    "print(\"\\nCross-Reference with Original Analysis:\")\n",
    "print(\"--------------------------------------\")\n",
    "print(f\"Original individual files count: 1,798\")\n",
    "print(f\"Current analysis:\")\n",
    "print(f\"- Unique CIKs with multiple companies: {len(multiple_company_ciks)}\")\n",
    "print(f\"- Total pairs with multiple companies: {paired_mda_df['has_multiple_companies'].sum()}\")\n",
    "print(f\"- Estimated individual files: {paired_mda_df['has_multiple_companies'].sum() * 2}\")  # multiply by 2 since each pair represents 2 files\n",
    "\n",
    "# Save the updated DataFrame\n",
    "output_path = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\paired_mda_reports_CLEANEDV4.csv'\n",
    "paired_mda_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nUpdated CSV saved to: {output_path}\")\n",
    "\n",
    "# Sample of pairs with multiple companies\n",
    "print(\"\\nSample of pairs with multiple companies:\")\n",
    "sample_pairs = paired_mda_df[paired_mda_df['has_multiple_companies']].sample(min(5, len(paired_mda_df[paired_mda_df['has_multiple_companies']])))\n",
    "for _, row in sample_pairs.iterrows():\n",
    "    print(f\"\\nCIK: {row['cik_number']}\")\n",
    "    print(f\"Number of Companies: {row['num_companies']}\")\n",
    "    print(f\"Current Filing Date: {row['current_filing_date']}\")\n",
    "    print(f\"Next Filing Date: {row['next_filing_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020 Pairs Analysis:\n",
      "-------------------\n",
      "Pairs with 2020 as current year: 8\n",
      "Pairs with 2020 as next year: 2145\n",
      "\n",
      "Sample of pairs with 2020 as current year:\n",
      "       cik_number current_filing_date next_filing_date  has_multiple_companies\n",
      "22689     1045942          2020-01-10       2020-12-23                   False\n",
      "23262     1394638          2020-04-01       2020-12-30                   False\n",
      "24261     1022505          2020-01-10       2020-12-29                   False\n",
      "25013     1536089          2020-01-15       2020-12-29                   False\n",
      "25650     1508348          2020-01-14       2020-12-21                   False\n",
      "\n",
      "Sample of pairs with 2020 as next year:\n",
      "    cik_number current_filing_date next_filing_date  has_multiple_companies\n",
      "21       46619          2019-12-19       2020-12-23                   False\n",
      "30      318300          2019-03-01       2020-03-03                   False\n",
      "46       72162          2019-03-11       2020-03-11                   False\n",
      "62     1038074          2019-01-30       2020-01-29                   False\n",
      "81       24741          2019-02-12       2020-02-18                   False\n",
      "\n",
      "Distribution of filing months for 2020 current year:\n",
      "current_filing_date\n",
      "1    7\n",
      "4    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of filing months for 2020 next year:\n",
      "next_filing_date\n",
      "1      34\n",
      "2     699\n",
      "3     772\n",
      "4     106\n",
      "5     116\n",
      "6      67\n",
      "7      28\n",
      "8      68\n",
      "9      60\n",
      "10     41\n",
      "11     77\n",
      "12     77\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert filing dates to datetime\n",
    "paired_mda_df['current_filing_date'] = pd.to_datetime(paired_mda_df['current_filing_date'])\n",
    "paired_mda_df['next_filing_date'] = pd.to_datetime(paired_mda_df['next_filing_date'])\n",
    "\n",
    "# Check for 2020 in either current or next filing dates\n",
    "pairs_2020_current = paired_mda_df[paired_mda_df['current_filing_date'].dt.year == 2020]\n",
    "pairs_2020_next = paired_mda_df[paired_mda_df['next_filing_date'].dt.year == 2020]\n",
    "\n",
    "print(\"\\n2020 Pairs Analysis:\")\n",
    "print(\"-------------------\")\n",
    "print(f\"Pairs with 2020 as current year: {len(pairs_2020_current)}\")\n",
    "print(f\"Pairs with 2020 as next year: {len(pairs_2020_next)}\")\n",
    "\n",
    "# Detailed look at 2020 pairs\n",
    "if len(pairs_2020_current) > 0:\n",
    "    print(\"\\nSample of pairs with 2020 as current year:\")\n",
    "    print(pairs_2020_current[['cik_number', 'current_filing_date', 'next_filing_date', 'has_multiple_companies']].head())\n",
    "\n",
    "if len(pairs_2020_next) > 0:\n",
    "    print(\"\\nSample of pairs with 2020 as next year:\")\n",
    "    print(pairs_2020_next[['cik_number', 'current_filing_date', 'next_filing_date', 'has_multiple_companies']].head())\n",
    "\n",
    "# Distribution of filing months for 2020\n",
    "if len(pairs_2020_current) > 0:\n",
    "    print(\"\\nDistribution of filing months for 2020 current year:\")\n",
    "    print(pairs_2020_current['current_filing_date'].dt.month.value_counts().sort_index())\n",
    "\n",
    "if len(pairs_2020_next) > 0:\n",
    "    print(\"\\nDistribution of filing months for 2020 next year:\")\n",
    "    print(pairs_2020_next['next_filing_date'].dt.month.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Data Check:\n",
      "Total rows: 27508\n",
      "Total rows with multiple companies: 654\n",
      "Unique CIKs with multiple companies: 85\n",
      "\n",
      "Dataset Statistics:\n",
      "Total pairs in dataset: 27508\n",
      "Total CIKs with multiple companies: 85\n",
      "DAPT data (2000-2010 + all multiple companies): 10719 pairs\n",
      "Training data (2011-2015.06, single company only): 8257 pairs\n",
      "Validation data (2015.07-2017.06, single company only): 3975 pairs\n",
      "Test data (2017.07+, single company only): 4557 pairs\n",
      "\n",
      "DAPT data date range:\n",
      "Current MDAs: 2001-01-10 00:00:00 to 2019-12-05 00:00:00\n",
      "Next MDAs: 2001-12-14 00:00:00 to 2020-12-07 00:00:00\n",
      "Number of unique companies: 2427\n",
      "Number of pairs with multiple companies: 654\n",
      "\n",
      "Year-Month distribution for current filings:\n",
      "current_filing_date  current_filing_date\n",
      "2001                 1                       19\n",
      "                     2                       25\n",
      "                     3                      373\n",
      "                     4                      132\n",
      "                     5                       17\n",
      "                                           ... \n",
      "2019                 3                        4\n",
      "                     4                        1\n",
      "                     5                        1\n",
      "                     10                       1\n",
      "                     12                       1\n",
      "Length: 181, dtype: int64\n",
      "\n",
      "Year-Month distribution for next filings:\n",
      "next_filing_date  next_filing_date\n",
      "2001              12                    1\n",
      "2002              1                    15\n",
      "                  2                    35\n",
      "                  3                   394\n",
      "                  4                   105\n",
      "                                     ... \n",
      "2020              3                     5\n",
      "                  4                     1\n",
      "                  5                     1\n",
      "                  10                    1\n",
      "                  12                    1\n",
      "Length: 183, dtype: int64\n",
      "\n",
      "Training data date range:\n",
      "Current MDAs: 2011-01-03 00:00:00 to 2015-06-30 00:00:00\n",
      "Next MDAs: 2011-11-22 00:00:00 to 2016-08-12 00:00:00\n",
      "Number of unique companies: 2633\n",
      "Number of pairs with multiple companies: 0\n",
      "\n",
      "Year-Month distribution for current filings:\n",
      "current_filing_date  current_filing_date\n",
      "2011                 1                       34\n",
      "                     2                      389\n",
      "                     3                      704\n",
      "                     4                       84\n",
      "                     5                       38\n",
      "                     6                       46\n",
      "                     7                       33\n",
      "                     8                       42\n",
      "                     9                       68\n",
      "                     10                      33\n",
      "                     11                      68\n",
      "                     12                      58\n",
      "2012                 1                       37\n",
      "                     2                      502\n",
      "                     3                      598\n",
      "                     4                      101\n",
      "                     5                       39\n",
      "                     6                       54\n",
      "                     7                       36\n",
      "                     8                       37\n",
      "                     9                       63\n",
      "                     10                      44\n",
      "                     11                      66\n",
      "                     12                      59\n",
      "2013                 1                       39\n",
      "                     2                      452\n",
      "                     3                      625\n",
      "                     4                      196\n",
      "                     5                       44\n",
      "                     6                       49\n",
      "                     7                       45\n",
      "                     8                       40\n",
      "                     9                       70\n",
      "                     10                      38\n",
      "                     11                      66\n",
      "                     12                      57\n",
      "2014                 1                       46\n",
      "                     2                      502\n",
      "                     3                      691\n",
      "                     4                      117\n",
      "                     5                       39\n",
      "                     6                       55\n",
      "                     7                       38\n",
      "                     8                       46\n",
      "                     9                       74\n",
      "                     10                      37\n",
      "                     11                      60\n",
      "                     12                      66\n",
      "2015                 1                       44\n",
      "                     2                      522\n",
      "                     3                      767\n",
      "                     4                      104\n",
      "                     5                       48\n",
      "                     6                       47\n",
      "dtype: int64\n",
      "\n",
      "Year-Month distribution for next filings:\n",
      "next_filing_date  next_filing_date\n",
      "2011              11                    1\n",
      "                  12                   10\n",
      "2012              1                    25\n",
      "                  2                   508\n",
      "                  3                   574\n",
      "                  4                    97\n",
      "                  5                    37\n",
      "                  6                    47\n",
      "                  7                    29\n",
      "                  8                    48\n",
      "                  9                    58\n",
      "                  10                   40\n",
      "                  11                   68\n",
      "                  12                   57\n",
      "2013              1                    36\n",
      "                  2                   433\n",
      "                  3                   588\n",
      "                  4                   179\n",
      "                  5                    38\n",
      "                  6                    47\n",
      "                  7                    44\n",
      "                  8                    39\n",
      "                  9                    67\n",
      "                  10                   39\n",
      "                  11                   67\n",
      "                  12                   51\n",
      "2014              1                    46\n",
      "                  2                   513\n",
      "                  3                   662\n",
      "                  4                   106\n",
      "                  5                    34\n",
      "                  6                    59\n",
      "                  7                    36\n",
      "                  8                    42\n",
      "                  9                    71\n",
      "                  10                   36\n",
      "                  11                   60\n",
      "                  12                   64\n",
      "2015              1                    43\n",
      "                  2                   504\n",
      "                  3                   713\n",
      "                  4                    99\n",
      "                  5                    37\n",
      "                  6                    48\n",
      "                  7                    41\n",
      "                  8                    46\n",
      "                  9                    73\n",
      "                  10                   36\n",
      "                  11                   65\n",
      "                  12                   65\n",
      "2016              1                    42\n",
      "                  2                   625\n",
      "                  3                   672\n",
      "                  4                    97\n",
      "                  5                    47\n",
      "                  6                    42\n",
      "                  7                     5\n",
      "                  8                     1\n",
      "dtype: int64\n",
      "\n",
      "Validation data date range:\n",
      "Current MDAs: 2015-07-06 00:00:00 to 2017-06-30 00:00:00\n",
      "Next MDAs: 2016-04-15 00:00:00 to 2018-08-14 00:00:00\n",
      "Number of unique companies: 2263\n",
      "Number of pairs with multiple companies: 0\n",
      "\n",
      "Year-Month distribution for current filings:\n",
      "current_filing_date  current_filing_date\n",
      "2015                 7                       39\n",
      "                     8                       45\n",
      "                     9                       71\n",
      "                     10                      41\n",
      "                     11                      64\n",
      "                     12                      64\n",
      "2016                 1                       43\n",
      "                     2                      657\n",
      "                     3                      721\n",
      "                     4                      108\n",
      "                     5                       45\n",
      "                     6                       51\n",
      "                     7                       31\n",
      "                     8                       49\n",
      "                     9                       77\n",
      "                     10                      33\n",
      "                     11                      75\n",
      "                     12                      74\n",
      "2017                 1                       33\n",
      "                     2                      599\n",
      "                     3                      823\n",
      "                     4                      135\n",
      "                     5                       41\n",
      "                     6                       56\n",
      "dtype: int64\n",
      "\n",
      "Year-Month distribution for next filings:\n",
      "next_filing_date  next_filing_date\n",
      "2016              4                     1\n",
      "                  6                     7\n",
      "                  7                    27\n",
      "                  8                    52\n",
      "                  9                    72\n",
      "                  10                   33\n",
      "                  11                   67\n",
      "                  12                   71\n",
      "2017              1                    34\n",
      "                  2                   599\n",
      "                  3                   775\n",
      "                  4                   124\n",
      "                  5                    40\n",
      "                  6                    51\n",
      "                  7                    29\n",
      "                  8                    52\n",
      "                  9                    64\n",
      "                  10                   41\n",
      "                  11                   78\n",
      "                  12                   68\n",
      "2018              1                    39\n",
      "                  2                   623\n",
      "                  3                   684\n",
      "                  4                   246\n",
      "                  5                    44\n",
      "                  6                    44\n",
      "                  7                     8\n",
      "                  8                     2\n",
      "dtype: int64\n",
      "\n",
      "Test data date range:\n",
      "Current MDAs: 2017-07-03 00:00:00 to 2020-04-01 00:00:00\n",
      "Next MDAs: 2018-06-04 00:00:00 to 2020-12-30 00:00:00\n",
      "Number of unique companies: 2575\n",
      "Number of pairs with multiple companies: 0\n",
      "\n",
      "Year-Month distribution for current filings:\n",
      "current_filing_date  current_filing_date\n",
      "2017                 7                       29\n",
      "                     8                       45\n",
      "                     9                       67\n",
      "                     10                      43\n",
      "                     11                      84\n",
      "                     12                      75\n",
      "2018                 1                       39\n",
      "                     2                      634\n",
      "                     3                      735\n",
      "                     4                      261\n",
      "                     5                       44\n",
      "                     6                       60\n",
      "                     7                       34\n",
      "                     8                       54\n",
      "                     9                       57\n",
      "                     10                      37\n",
      "                     11                      86\n",
      "                     12                      72\n",
      "2019                 1                       29\n",
      "                     2                      614\n",
      "                     3                      749\n",
      "                     4                      256\n",
      "                     5                       50\n",
      "                     6                       44\n",
      "                     7                       45\n",
      "                     8                       65\n",
      "                     9                       59\n",
      "                     10                      36\n",
      "                     11                      78\n",
      "                     12                      68\n",
      "2020                 1                        7\n",
      "                     4                        1\n",
      "dtype: int64\n",
      "\n",
      "Year-Month distribution for next filings:\n",
      "next_filing_date  next_filing_date\n",
      "2018              6                     7\n",
      "                  7                    25\n",
      "                  8                    51\n",
      "                  9                    61\n",
      "                  10                   41\n",
      "                  11                   87\n",
      "                  12                   72\n",
      "2019              1                    39\n",
      "                  2                   685\n",
      "                  3                   700\n",
      "                  4                   244\n",
      "                  5                    47\n",
      "                  6                    44\n",
      "                  7                    39\n",
      "                  8                    68\n",
      "                  9                    58\n",
      "                  10                   39\n",
      "                  11                   79\n",
      "                  12                   72\n",
      "2020              1                    33\n",
      "                  2                   663\n",
      "                  3                   767\n",
      "                  4                   105\n",
      "                  5                   115\n",
      "                  6                    67\n",
      "                  7                    28\n",
      "                  8                    68\n",
      "                  9                    60\n",
      "                  10                   40\n",
      "                  11                   77\n",
      "                  12                   76\n",
      "dtype: int64\n",
      "\n",
      "Verifying no temporal overlap between splits...\n",
      "No overlap between Training and Validation\n",
      "No overlap between Validation and Test\n",
      "\n",
      "Verification: All 654 multiple-company pairs are in DAPT dataset\n",
      "\n",
      "Total rows: 27508\n",
      "Total assigned: 27508\n",
      "All rows assigned successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the cleaned data with multiple-company markings\n",
    "input_path = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\paired_mda_reports_CLEANEDV4.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "print(\"\\nInitial Data Check:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total rows with multiple companies: {df['has_multiple_companies'].sum()}\")\n",
    "print(f\"Unique CIKs with multiple companies: {df[df['has_multiple_companies']]['cik_number'].nunique()}\")\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['current_filing_date'] = pd.to_datetime(df['current_filing_date'])\n",
    "df['next_filing_date'] = pd.to_datetime(df['next_filing_date'])\n",
    "\n",
    "# Add year-month columns for transition handling\n",
    "df['current_ym'] = pd.to_datetime(df['current_filing_date'].dt.strftime('%Y-%m-01'))\n",
    "df['next_ym'] = pd.to_datetime(df['next_filing_date'].dt.strftime('%Y-%m-01'))\n",
    "\n",
    "# Define period boundaries\n",
    "TRAIN_END = pd.Timestamp('2015-06-30')    # Training cutoff\n",
    "VAL_END = pd.Timestamp('2017-06-30')      # Validation cutoff\n",
    "\n",
    "# Identify all CIKs with multiple companies\n",
    "multiple_company_ciks = set(df[df['has_multiple_companies']]['cik_number'])\n",
    "\n",
    "# Create the DAPT dataset (2000-2010 + all multiple companies)\n",
    "dapt_data = df[\n",
    "    (df['current_filing_date'].dt.year <= 2010) |\n",
    "    (df['cik_number'].isin(multiple_company_ciks))\n",
    "]\n",
    "\n",
    "# Remaining data (excluding DAPT data)\n",
    "remaining_data = df[\n",
    "    ~(df['current_filing_date'].dt.year <= 2010) &\n",
    "    ~df['cik_number'].isin(multiple_company_ciks)\n",
    "]\n",
    "\n",
    "# Split remaining data with clean cutoffs\n",
    "train_data = remaining_data[\n",
    "    (remaining_data['current_filing_date'] <= TRAIN_END)\n",
    "]\n",
    "\n",
    "val_data = remaining_data[\n",
    "    (remaining_data['current_filing_date'] > TRAIN_END) &\n",
    "    (remaining_data['current_filing_date'] <= VAL_END)\n",
    "]\n",
    "\n",
    "test_data = remaining_data[\n",
    "    (remaining_data['current_filing_date'] > VAL_END)\n",
    "]\n",
    "\n",
    "# Print comprehensive statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total pairs in dataset: {len(df)}\")\n",
    "print(f\"Total CIKs with multiple companies: {len(multiple_company_ciks)}\")\n",
    "print(f\"DAPT data (2000-2010 + all multiple companies): {len(dapt_data)} pairs\")\n",
    "print(f\"Training data (2011-2015.06, single company only): {len(train_data)} pairs\")\n",
    "print(f\"Validation data (2015.07-2017.06, single company only): {len(val_data)} pairs\")\n",
    "print(f\"Test data (2017.07+, single company only): {len(test_data)} pairs\")\n",
    "\n",
    "def check_date_ranges(dataset, name):\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\n{name} date range:\")\n",
    "        print(f\"Current MDAs: {dataset['current_filing_date'].min()} to {dataset['current_filing_date'].max()}\")\n",
    "        print(f\"Next MDAs: {dataset['next_filing_date'].min()} to {dataset['next_filing_date'].max()}\")\n",
    "        print(f\"Number of unique companies: {dataset['company_name'].nunique()}\")\n",
    "        print(f\"Number of pairs with multiple companies: {dataset['has_multiple_companies'].sum()}\")\n",
    "        print(\"\\nYear-Month distribution for current filings:\")\n",
    "        print(dataset.groupby([dataset['current_filing_date'].dt.year, \n",
    "                             dataset['current_filing_date'].dt.month]).size().sort_index())\n",
    "        print(\"\\nYear-Month distribution for next filings:\")\n",
    "        print(dataset.groupby([dataset['next_filing_date'].dt.year, \n",
    "                             dataset['next_filing_date'].dt.month]).size().sort_index())\n",
    "    else:\n",
    "        print(f\"\\n{name} is empty\")\n",
    "\n",
    "# Check each split\n",
    "check_date_ranges(dapt_data, \"DAPT data\")\n",
    "check_date_ranges(train_data, \"Training data\")\n",
    "check_date_ranges(val_data, \"Validation data\")\n",
    "check_date_ranges(test_data, \"Test data\")\n",
    "\n",
    "# Verify no temporal overlap\n",
    "print(\"\\nVerifying no temporal overlap between splits...\")\n",
    "def verify_no_overlap(df1, df1_name, df2, df2_name):\n",
    "    next_mdas_1 = set(df1['current_filing_date'])\n",
    "    current_mdas_2 = set(df2['current_filing_date'])\n",
    "    overlap = next_mdas_1.intersection(current_mdas_2)\n",
    "    if overlap:\n",
    "        print(f\"Warning: Found {len(overlap)} overlapping dates between {df1_name} and {df2_name}\")\n",
    "        print(\"Sample overlapping dates:\", sorted(overlap)[:5], \"...\")\n",
    "    else:\n",
    "        print(f\"No overlap between {df1_name} and {df2_name}\")\n",
    "\n",
    "verify_no_overlap(train_data, \"Training\", val_data, \"Validation\")\n",
    "verify_no_overlap(val_data, \"Validation\", test_data, \"Test\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_base = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\Model Data'\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# Save the splits\n",
    "dapt_data.to_csv(os.path.join(output_base, 'dapt_data.csv'), index=False)\n",
    "train_data.to_csv(os.path.join(output_base, 'train_data.csv'), index=False)\n",
    "val_data.to_csv(os.path.join(output_base, 'val_data.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_base, 'test_data.csv'), index=False)\n",
    "\n",
    "# Verify all multiple-company pairs are in DAPT\n",
    "multiple_company_pairs = df['has_multiple_companies'].sum()\n",
    "multiple_company_pairs_in_dapt = dapt_data['has_multiple_companies'].sum()\n",
    "assert multiple_company_pairs == multiple_company_pairs_in_dapt, \"Not all multiple-company pairs are in DAPT dataset\"\n",
    "print(f\"\\nVerification: All {multiple_company_pairs} multiple-company pairs are in DAPT dataset\")\n",
    "\n",
    "# Verify all data is assigned\n",
    "total_assigned = len(dapt_data) + len(train_data) + len(val_data) + len(test_data)\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "print(f\"Total assigned: {total_assigned}\")\n",
    "if total_assigned != len(df):\n",
    "    print(f\"Warning: {len(df) - total_assigned} rows unassigned!\")\n",
    "else:\n",
    "    print(\"All rows assigned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of multiple-company files by year:\n",
      "Year 2001: 58 files (3.96%)\n",
      "Year 2002: 19 files (1.30%)\n",
      "Year 2003: 20 files (1.36%)\n",
      "Year 2004: 28 files (1.91%)\n",
      "Year 2005: 41 files (2.80%)\n",
      "Year 2006: 39 files (2.66%)\n",
      "Year 2007: 35 files (2.39%)\n",
      "Year 2008: 49 files (3.34%)\n",
      "Year 2009: 41 files (2.80%)\n",
      "Year 2010: 30 files (2.05%)\n",
      "Year 2011: 37 files (2.52%)\n",
      "Year 2012: 48 files (3.27%)\n",
      "Year 2013: 41 files (2.80%)\n",
      "Year 2014: 128 files (8.73%)\n",
      "Year 2015: 112 files (7.64%)\n",
      "Year 2016: 127 files (8.66%)\n",
      "Year 2017: 140 files (9.55%)\n",
      "Year 2018: 159 files (10.85%)\n",
      "Year 2019: 157 files (10.71%)\n",
      "Year 2020: 157 files (10.71%)\n",
      "\n",
      "Year Range Summary:\n",
      "Earliest year: 2001\n",
      "Latest year: 2020\n",
      "Number of years covered: 20\n",
      "\n",
      "Detailed year analysis saved to: C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\multiple_companies_year_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# After your existing code, add:\n",
    "\n",
    "# Extract years from file paths of companies found in paired reports\n",
    "years_analysis = {}\n",
    "for company in found_companies:\n",
    "    file_path = company['file_path']\n",
    "    \n",
    "    # Extract year from file path using regex\n",
    "    year_match = re.search(r'[12]\\d{3}', os.path.basename(file_path))\n",
    "    if year_match:\n",
    "        year = year_match.group()\n",
    "        years_analysis[year] = years_analysis.get(year, 0) + 1\n",
    "\n",
    "# Sort years and print distribution\n",
    "print(\"\\nDistribution of multiple-company files by year:\")\n",
    "for year in sorted(years_analysis.keys()):\n",
    "    count = years_analysis[year]\n",
    "    percentage = (count / len(found_companies)) * 100\n",
    "    print(f\"Year {year}: {count} files ({percentage:.2f}%)\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nYear Range Summary:\")\n",
    "print(f\"Earliest year: {min(years_analysis.keys())}\")\n",
    "print(f\"Latest year: {max(years_analysis.keys())}\")\n",
    "print(f\"Number of years covered: {len(years_analysis)}\")\n",
    "\n",
    "# Optionally, create a more detailed DataFrame for analysis\n",
    "year_analysis_df = pd.DataFrame([\n",
    "    {'year': year, 'count': count, 'percentage': (count / len(found_companies)) * 100}\n",
    "    for year, count in years_analysis.items()\n",
    "])\n",
    "year_analysis_df = year_analysis_df.sort_values('year')\n",
    "\n",
    "# Save the year analysis to a CSV\n",
    "year_analysis_path = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\multiple_companies_year_analysis.csv'\n",
    "year_analysis_df.to_csv(year_analysis_path, index=False)\n",
    "print(f\"\\nDetailed year analysis saved to: {year_analysis_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year-by-Year Comparison (2001-2019):\n",
      "------------------------------------------------\n",
      "Year 2001:\n",
      "  Total files in CSV: 733\n",
      "  Files with multiple companies: 74\n",
      "  Percentage: 10.10%\n",
      "------------------------------------------------\n",
      "Year 2002:\n",
      "  Total files in CSV: 783\n",
      "  Files with multiple companies: 23\n",
      "  Percentage: 2.94%\n",
      "------------------------------------------------\n",
      "Year 2003:\n",
      "  Total files in CSV: 818\n",
      "  Files with multiple companies: 22\n",
      "  Percentage: 2.69%\n",
      "------------------------------------------------\n",
      "Year 2004:\n",
      "  Total files in CSV: 864\n",
      "  Files with multiple companies: 41\n",
      "  Percentage: 4.75%\n",
      "------------------------------------------------\n",
      "Year 2005:\n",
      "  Total files in CSV: 908\n",
      "  Files with multiple companies: 61\n",
      "  Percentage: 6.72%\n",
      "------------------------------------------------\n",
      "Year 2006:\n",
      "  Total files in CSV: 981\n",
      "  Files with multiple companies: 60\n",
      "  Percentage: 6.12%\n",
      "------------------------------------------------\n",
      "Year 2007:\n",
      "  Total files in CSV: 1066\n",
      "  Files with multiple companies: 48\n",
      "  Percentage: 4.50%\n",
      "------------------------------------------------\n",
      "Year 2008:\n",
      "  Total files in CSV: 1194\n",
      "  Files with multiple companies: 74\n",
      "  Percentage: 6.20%\n",
      "------------------------------------------------\n",
      "Year 2009:\n",
      "  Total files in CSV: 1466\n",
      "  Files with multiple companies: 63\n",
      "  Percentage: 4.30%\n",
      "------------------------------------------------\n",
      "Year 2010:\n",
      "  Total files in CSV: 1527\n",
      "  Files with multiple companies: 44\n",
      "  Percentage: 2.88%\n",
      "------------------------------------------------\n",
      "Year 2011:\n",
      "  Total files in CSV: 1634\n",
      "  Files with multiple companies: 51\n",
      "  Percentage: 3.12%\n",
      "------------------------------------------------\n",
      "Year 2012:\n",
      "  Total files in CSV: 1672\n",
      "  Files with multiple companies: 64\n",
      "  Percentage: 3.83%\n",
      "------------------------------------------------\n",
      "Year 2013:\n",
      "  Total files in CSV: 1751\n",
      "  Files with multiple companies: 65\n",
      "  Percentage: 3.71%\n",
      "------------------------------------------------\n",
      "Year 2014:\n",
      "  Total files in CSV: 1812\n",
      "  Files with multiple companies: 172\n",
      "  Percentage: 9.49%\n",
      "------------------------------------------------\n",
      "Year 2015:\n",
      "  Total files in CSV: 1900\n",
      "  Files with multiple companies: 156\n",
      "  Percentage: 8.21%\n",
      "------------------------------------------------\n",
      "Year 2016:\n",
      "  Total files in CSV: 2009\n",
      "  Files with multiple companies: 176\n",
      "  Percentage: 8.76%\n",
      "------------------------------------------------\n",
      "Year 2017:\n",
      "  Total files in CSV: 2079\n",
      "  Files with multiple companies: 183\n",
      "  Percentage: 8.80%\n",
      "------------------------------------------------\n",
      "Year 2018:\n",
      "  Total files in CSV: 2164\n",
      "  Files with multiple companies: 207\n",
      "  Percentage: 9.57%\n",
      "------------------------------------------------\n",
      "Year 2019:\n",
      "  Total files in CSV: 2139\n",
      "  Files with multiple companies: 214\n",
      "  Percentage: 10.00%\n",
      "------------------------------------------------\n",
      "\n",
      "Updated analysis saved to: C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\filing_dates_analysis_adjusted.csv\n",
      "\n",
      "Summary Statistics (2001-2019):\n",
      "Total files in CSV: 27500\n",
      "Total multiple-company files: 1798\n",
      "Average percentage across years: 6.14%\n",
      "Maximum percentage in a single year: 10.10%\n",
      "Minimum percentage in a single year: 2.69%\n"
     ]
    }
   ],
   "source": [
    "# Extract years from the CSV file's filing dates and from the file paths\n",
    "csv_cutoff_date = pd.to_datetime('2020-04-01')  # The latest date in your CSV\n",
    "\n",
    "# Get CSV years distribution excluding partial 2020 data\n",
    "csv_dates = pd.to_datetime(paired_mda_df['current_filing_date'])\n",
    "csv_years = csv_dates[csv_dates < csv_cutoff_date].dt.year.value_counts().sort_index()\n",
    "\n",
    "# Reanalyze multiple-company files with date cutoff\n",
    "multiple_company_dates = []\n",
    "for file_info in multiple_company_files:\n",
    "    try:\n",
    "        with open(file_info['file_path'], 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            filing_date = extract_filing_date(content)\n",
    "            if filing_date != \"Unknown\":\n",
    "                file_date = pd.to_datetime(filing_date)\n",
    "                if file_date < csv_cutoff_date:  # Exclude 2020 data\n",
    "                    multiple_company_dates.append({\n",
    "                        'file_path': file_info['file_path'],\n",
    "                        'filing_date': filing_date,\n",
    "                        'year_from_path': re.search(r'[12]\\d{3}', os.path.basename(file_info['file_path'])).group()\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_info['file_path']}: {e}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "multiple_dates_df = pd.DataFrame(multiple_company_dates)\n",
    "multiple_dates_df['filing_year'] = pd.to_datetime(multiple_dates_df['filing_date']).dt.year\n",
    "\n",
    "# Create updated years_analysis dictionary\n",
    "updated_years_analysis = multiple_dates_df['filing_year'].value_counts().to_dict()\n",
    "\n",
    "# Create comparison DataFrame with cutoff-adjusted data (excluding 2020)\n",
    "comparison_data = []\n",
    "for year in sorted(set(csv_years.index)):  # Only use complete years\n",
    "    if year < 2020:  # Exclude 2020 entirely\n",
    "        total_files = csv_years.get(year, 0)\n",
    "        multiple_files = updated_years_analysis.get(year, 0)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'year': year,\n",
    "            'total_files': total_files,\n",
    "            'multiple_company_files': multiple_files,\n",
    "            'percentage': (multiple_files / total_files * 100) if total_files > 0 else 0\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Print the comparison\n",
    "print(\"\\nYear-by-Year Comparison (2001-2019):\")\n",
    "print(\"------------------------------------------------\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"Year {int(row['year'])}:\")\n",
    "    print(f\"  Total files in CSV: {int(row['total_files'])}\")\n",
    "    print(f\"  Files with multiple companies: {int(row['multiple_company_files'])}\")\n",
    "    print(f\"  Percentage: {row['percentage']:.2f}%\")\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "# Save updated analysis\n",
    "analysis_path = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\filing_dates_analysis_adjusted.csv'\n",
    "comparison_df.to_csv(analysis_path, index=False)\n",
    "print(f\"\\nUpdated analysis saved to: {analysis_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics (2001-2019):\")\n",
    "print(f\"Total files in CSV: {comparison_df['total_files'].sum()}\")\n",
    "print(f\"Total multiple-company files: {comparison_df['multiple_company_files'].sum()}\")\n",
    "print(f\"Average percentage across years: {comparison_df['percentage'].mean():.2f}%\")\n",
    "print(f\"Maximum percentage in a single year: {comparison_df['percentage'].max():.2f}%\")\n",
    "print(f\"Minimum percentage in a single year: {comparison_df['percentage'].min():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis of Multiple-Company Files:\n",
      "-----------------------------------\n",
      "\n",
      "Comparison of file path years vs filing dates:\n",
      "filing_year     2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  \\\n",
      "year_from_path                                                               \n",
      "2001              74     0     0     0     0     0     0     0     0     0   \n",
      "2002               0    23     0     0     0     0     0     0     0     0   \n",
      "2003               0     0    22     0     0     0     0     0     0     0   \n",
      "2004               0     0     0    41     0     0     0     0     0     0   \n",
      "2005               0     0     0     0    61     0     0     0     0     0   \n",
      "2006               0     0     0     0     0    60     0     0     0     0   \n",
      "2007               0     0     0     0     0     0    48     0     0     0   \n",
      "2008               0     0     0     0     0     0     0    74     0     0   \n",
      "2009               0     0     0     0     0     0     0     0    63     0   \n",
      "2010               0     0     0     0     0     0     0     0     0    44   \n",
      "2011               0     0     0     0     0     0     0     0     0     0   \n",
      "2012               0     0     0     0     0     0     0     0     0     0   \n",
      "2013               0     0     0     0     0     0     0     0     0     0   \n",
      "2014               0     0     0     0     0     0     0     0     0     0   \n",
      "2015               0     0     0     0     0     0     0     0     0     0   \n",
      "2016               0     0     0     0     0     0     0     0     0     0   \n",
      "2017               0     0     0     0     0     0     0     0     0     0   \n",
      "2018               0     0     0     0     0     0     0     0     0     0   \n",
      "2019               0     0     0     0     0     0     0     0     0     0   \n",
      "2020               0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "filing_year     2011  2012  2013  2014  2015  2016  2017  2018  2019  2020  \n",
      "year_from_path                                                              \n",
      "2001               0     0     0     0     0     0     0     0     0     0  \n",
      "2002               0     0     0     0     0     0     0     0     0     0  \n",
      "2003               0     0     0     0     0     0     0     0     0     0  \n",
      "2004               0     0     0     0     0     0     0     0     0     0  \n",
      "2005               0     0     0     0     0     0     0     0     0     0  \n",
      "2006               0     0     0     0     0     0     0     0     0     0  \n",
      "2007               0     0     0     0     0     0     0     0     0     0  \n",
      "2008               0     0     0     0     0     0     0     0     0     0  \n",
      "2009               0     0     0     0     0     0     0     0     0     0  \n",
      "2010               0     0     0     0     0     0     0     0     0     0  \n",
      "2011              51     0     0     0     0     0     0     0     0     0  \n",
      "2012               0    64     0     0     0     0     0     0     0     0  \n",
      "2013               0     0    65     0     0     0     0     0     0     0  \n",
      "2014               0     0     0   172     0     0     0     0     0     0  \n",
      "2015               0     0     0     0   156     0     0     0     0     0  \n",
      "2016               0     0     0     0     0   176     0     0     0     0  \n",
      "2017               0     0     0     0     0     0   183     0     0     0  \n",
      "2018               0     0     0     0     0     0     0   207     0     0  \n",
      "2019               0     0     0     0     0     0     0     0   214     0  \n",
      "2020               0     0     0     0     0     0     0     0     0   222  \n",
      "\n",
      "Analysis of CSV Filing Dates:\n",
      "-----------------------------\n",
      "\n",
      "Distribution of current_filing_date by year:\n",
      "current_filing_date\n",
      "2001     733\n",
      "2002     783\n",
      "2003     818\n",
      "2004     864\n",
      "2005     908\n",
      "2006     981\n",
      "2007    1066\n",
      "2008    1194\n",
      "2009    1466\n",
      "2010    1527\n",
      "2011    1634\n",
      "2012    1672\n",
      "2013    1751\n",
      "2014    1812\n",
      "2015    1900\n",
      "2016    2009\n",
      "2017    2079\n",
      "2018    2164\n",
      "2019    2139\n",
      "2020       8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range in CSV:\n",
      "Earliest date: 2001-01-10 00:00:00\n",
      "Latest date: 2020-04-01 00:00:00\n",
      "\n",
      "Sample of filing dates from CSV:\n",
      "0    2001-03-12\n",
      "1    2002-03-08\n",
      "2    2001-01-25\n",
      "3    2002-01-29\n",
      "4    2003-01-22\n",
      "Name: current_filing_date, dtype: object\n",
      "\n",
      "Date range for next_filing_date:\n",
      "Earliest next date: 2001-12-14 00:00:00\n",
      "Latest next date: 2020-12-30 00:00:00\n",
      "\n",
      "Detailed analysis saved to: C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\filing_dates_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_filing_date(content):\n",
    "    \"\"\"\n",
    "    Extracts the filing date from the file content.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The full text of the 10-K document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted filing date in YYYY-MM-DD format or \"Unknown\" if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r'FILED AS OF DATE:\\s*(\\d{8})', content)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# First, let's analyze the filing dates of the multiple-company files\n",
    "multiple_company_dates = []\n",
    "for file_info in multiple_company_files:\n",
    "    try:\n",
    "        with open(file_info['file_path'], 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            filing_date = extract_filing_date(content)\n",
    "            if filing_date != \"Unknown\":\n",
    "                multiple_company_dates.append({\n",
    "                    'file_path': file_info['file_path'],\n",
    "                    'filing_date': filing_date,\n",
    "                    'year_from_path': re.search(r'[12]\\d{3}', os.path.basename(file_info['file_path'])).group()\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_info['file_path']}: {e}\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "multiple_dates_df = pd.DataFrame(multiple_company_dates)\n",
    "multiple_dates_df['filing_year'] = pd.to_datetime(multiple_dates_df['filing_date']).dt.year\n",
    "\n",
    "# Compare years from file paths vs actual filing dates\n",
    "print(\"\\nAnalysis of Multiple-Company Files:\")\n",
    "print(\"-----------------------------------\")\n",
    "print(\"\\nComparison of file path years vs filing dates:\")\n",
    "year_comparison = multiple_dates_df.groupby(['year_from_path', 'filing_year']).size().unstack(fill_value=0)\n",
    "print(year_comparison)\n",
    "\n",
    "# Analyze CSV filing dates\n",
    "print(\"\\nAnalysis of CSV Filing Dates:\")\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nDistribution of current_filing_date by year:\")\n",
    "csv_dates = pd.to_datetime(paired_mda_df['current_filing_date'])\n",
    "print(csv_dates.dt.year.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDate range in CSV:\")\n",
    "print(f\"Earliest date: {csv_dates.min()}\")\n",
    "print(f\"Latest date: {csv_dates.max()}\")\n",
    "\n",
    "# Check for any \"Unknown\" dates\n",
    "unknown_dates = paired_mda_df[paired_mda_df['current_filing_date'] == \"Unknown\"]\n",
    "if len(unknown_dates) > 0:\n",
    "    print(f\"\\nFound {len(unknown_dates)} entries with 'Unknown' filing dates in CSV\")\n",
    "\n",
    "# Check for potential date format issues\n",
    "print(\"\\nSample of filing dates from CSV:\")\n",
    "print(paired_mda_df['current_filing_date'].head())\n",
    "\n",
    "# Compare with next_filing_dates\n",
    "print(\"\\nDate range for next_filing_date:\")\n",
    "next_dates = pd.to_datetime(paired_mda_df['next_filing_date'])\n",
    "print(f\"Earliest next date: {next_dates.min()}\")\n",
    "print(f\"Latest next date: {next_dates.max()}\")\n",
    "\n",
    "# Save detailed analysis to CSV\n",
    "analysis_df = pd.DataFrame({\n",
    "    'year': range(2001, 2021),\n",
    "    'csv_current_dates': csv_dates.dt.year.value_counts(),\n",
    "    'csv_next_dates': next_dates.dt.year.value_counts(),\n",
    "    'multiple_company_files': [years_analysis.get(str(year), 0) for year in range(2001, 2021)]\n",
    "})\n",
    "analysis_df = analysis_df.fillna(0)\n",
    "\n",
    "analysis_path = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\filing_dates_analysis.csv'\n",
    "analysis_df.to_csv(analysis_path, index=False)\n",
    "print(f\"\\nDetailed analysis saved to: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV:\n",
      "['current_filename', 'next_filename', 'company_name', 'cik_number', 'current_filing_date', 'next_filing_date', 'current_mda_content', 'next_mda_content', 'time_difference']\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27508 entries, 0 to 27507\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   current_filename     27508 non-null  object\n",
      " 1   next_filename        27508 non-null  object\n",
      " 2   company_name         27508 non-null  object\n",
      " 3   cik_number           27508 non-null  int64 \n",
      " 4   current_filing_date  27508 non-null  object\n",
      " 5   next_filing_date     27508 non-null  object\n",
      " 6   current_mda_content  27508 non-null  object\n",
      " 7   next_mda_content     27508 non-null  object\n",
      " 8   time_difference      27508 non-null  int64 \n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\paired_mda_reports_CLEANED.csv')\n",
    "\n",
    "# Display column names\n",
    "print(\"Column names in the CSV:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
