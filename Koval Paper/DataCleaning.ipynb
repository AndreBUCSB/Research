{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the cleaned data with multiple-company markings\n",
    "input_path = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\paired_mda_reports_CLEANEDV4.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "print(\"\\nInitial Data Check:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total rows with multiple companies: {df['has_multiple_companies'].sum()}\")\n",
    "print(f\"Unique CIKs with multiple companies: {df[df['has_multiple_companies']]['cik_number'].nunique()}\")\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['current_filing_date'] = pd.to_datetime(df['current_filing_date'])\n",
    "df['next_filing_date'] = pd.to_datetime(df['next_filing_date'])\n",
    "\n",
    "# Add year-month columns for transition handling\n",
    "df['current_ym'] = pd.to_datetime(df['current_filing_date'].dt.strftime('%Y-%m-01'))\n",
    "df['next_ym'] = pd.to_datetime(df['next_filing_date'].dt.strftime('%Y-%m-01'))\n",
    "\n",
    "# Define period boundaries\n",
    "TRAIN_END = pd.Timestamp('2015-06-30')    # Training cutoff\n",
    "VAL_END = pd.Timestamp('2017-06-30')      # Validation cutoff\n",
    "\n",
    "# Identify all CIKs with multiple companies\n",
    "multiple_company_ciks = set(df[df['has_multiple_companies']]['cik_number'])\n",
    "\n",
    "# Create the DAPT dataset (2000-2010 + all multiple companies)\n",
    "dapt_data = df[\n",
    "    (df['current_filing_date'].dt.year <= 2010) |\n",
    "    (df['cik_number'].isin(multiple_company_ciks))\n",
    "]\n",
    "\n",
    "# Remaining data (excluding DAPT data)\n",
    "remaining_data = df[\n",
    "    ~(df['current_filing_date'].dt.year <= 2010) &\n",
    "    ~df['cik_number'].isin(multiple_company_ciks)\n",
    "]\n",
    "\n",
    "# Split remaining data with clean cutoffs\n",
    "train_data = remaining_data[\n",
    "    (remaining_data['current_filing_date'] <= TRAIN_END)\n",
    "]\n",
    "\n",
    "val_data = remaining_data[\n",
    "    (remaining_data['current_filing_date'] > TRAIN_END) &\n",
    "    (remaining_data['current_filing_date'] <= VAL_END)\n",
    "]\n",
    "\n",
    "test_data = remaining_data[\n",
    "    (remaining_data['current_filing_date'] > VAL_END)\n",
    "]\n",
    "\n",
    "# Print comprehensive statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total pairs in dataset: {len(df)}\")\n",
    "print(f\"Total CIKs with multiple companies: {len(multiple_company_ciks)}\")\n",
    "print(f\"DAPT data (2000-2010 + all multiple companies): {len(dapt_data)} pairs\")\n",
    "print(f\"Training data (2011-2015.06, single company only): {len(train_data)} pairs\")\n",
    "print(f\"Validation data (2015.07-2017.06, single company only): {len(val_data)} pairs\")\n",
    "print(f\"Test data (2017.07+, single company only): {len(test_data)} pairs\")\n",
    "\n",
    "def check_date_ranges(dataset, name):\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\n{name} date range:\")\n",
    "        print(f\"Current MDAs: {dataset['current_filing_date'].min()} to {dataset['current_filing_date'].max()}\")\n",
    "        print(f\"Next MDAs: {dataset['next_filing_date'].min()} to {dataset['next_filing_date'].max()}\")\n",
    "        print(f\"Number of unique companies: {dataset['company_name'].nunique()}\")\n",
    "        print(f\"Number of pairs with multiple companies: {dataset['has_multiple_companies'].sum()}\")\n",
    "        print(\"\\nYear-Month distribution for current filings:\")\n",
    "        print(dataset.groupby([dataset['current_filing_date'].dt.year, \n",
    "                             dataset['current_filing_date'].dt.month]).size().sort_index())\n",
    "        print(\"\\nYear-Month distribution for next filings:\")\n",
    "        print(dataset.groupby([dataset['next_filing_date'].dt.year, \n",
    "                             dataset['next_filing_date'].dt.month]).size().sort_index())\n",
    "    else:\n",
    "        print(f\"\\n{name} is empty\")\n",
    "\n",
    "# Check each split\n",
    "check_date_ranges(dapt_data, \"DAPT data\")\n",
    "check_date_ranges(train_data, \"Training data\")\n",
    "check_date_ranges(val_data, \"Validation data\")\n",
    "check_date_ranges(test_data, \"Test data\")\n",
    "\n",
    "# Verify no temporal overlap\n",
    "print(\"\\nVerifying no temporal overlap between splits...\")\n",
    "def verify_no_overlap(df1, df1_name, df2, df2_name):\n",
    "    next_mdas_1 = set(df1['current_filing_date'])\n",
    "    current_mdas_2 = set(df2['current_filing_date'])\n",
    "    overlap = next_mdas_1.intersection(current_mdas_2)\n",
    "    if overlap:\n",
    "        print(f\"Warning: Found {len(overlap)} overlapping dates between {df1_name} and {df2_name}\")\n",
    "        print(\"Sample overlapping dates:\", sorted(overlap)[:5], \"...\")\n",
    "    else:\n",
    "        print(f\"No overlap between {df1_name} and {df2_name}\")\n",
    "\n",
    "verify_no_overlap(train_data, \"Training\", val_data, \"Validation\")\n",
    "verify_no_overlap(val_data, \"Validation\", test_data, \"Test\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_base = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\Model Data'\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# Save the splits\n",
    "dapt_data.to_csv(os.path.join(output_base, 'dapt_data.csv'), index=False)\n",
    "train_data.to_csv(os.path.join(output_base, 'train_data.csv'), index=False)\n",
    "val_data.to_csv(os.path.join(output_base, 'val_data.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_base, 'test_data.csv'), index=False)\n",
    "\n",
    "# Verify all multiple-company pairs are in DAPT\n",
    "multiple_company_pairs = df['has_multiple_companies'].sum()\n",
    "multiple_company_pairs_in_dapt = dapt_data['has_multiple_companies'].sum()\n",
    "assert multiple_company_pairs == multiple_company_pairs_in_dapt, \"Not all multiple-company pairs are in DAPT dataset\"\n",
    "print(f\"\\nVerification: All {multiple_company_pairs} multiple-company pairs are in DAPT dataset\")\n",
    "\n",
    "# Verify all data is assigned\n",
    "total_assigned = len(dapt_data) + len(train_data) + len(val_data) + len(test_data)\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "print(f\"Total assigned: {total_assigned}\")\n",
    "if total_assigned != len(df):\n",
    "    print(f\"Warning: {len(df) - total_assigned} rows unassigned!\")\n",
    "else:\n",
    "    print(\"All rows assigned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abbra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MDAs into sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MDAs: 100%|██████████| 10719/10719 [07:43<00:00, 23.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total individual sentences: 4895131\n",
      "Total sentence pairs: 4733770\n",
      "\n",
      "Quality Check:\n",
      "Average sentence length: 25.236633095212365\n",
      "Shortest sentence: See page 10 for further detail.\n",
      "Longest sentence: Manufacture of electromagnetic interference and radio frequency interference shielding for primarily communications, computer and aerospace applications Santa Barbara Infrared, Inc. (SBIR)............... Design and manufacture of aerospace and defense electronically controlled infrared simulation and test equipment Trilectron Industries, Inc., formerly a part of the ETG which designed and manufactured electronically controlled ground support equipment for aircraft, was sold in September 2000.\n",
      "\n",
      "Random Sample of Clean Sentences:\n",
      "1. In connection with these actions, the Company took charges of 18.5 million and 7.7 million, respectively.\n",
      "2. The estimated fair value of our net assets is calculated based on the difference between the fair value of our assets and the fair value of our liabilities, adjusted for noncontrolling interests.\n",
      "3. For the fiscal year ended December 29, 2002 basic and diluted losses per common share were 1.08.\n",
      "4. OpenSRS Email Service Cost of revenues for email services are payable to thirdparty providers for licensing and royalty costs related to the provision of certain components of our email services.\n",
      "5. Our business is focused on engineering, designing, and building equipment, and installing systems that capture, clean and destroy airborne contaminants from industrial facilities as well as equipment that controls emissions from such facilities.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Modified path for input\n",
    "dapt_data = pd.read_csv(r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\Model Data\\dapt_data.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text before sentence splitting\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove table-like content\n",
    "    text = re.sub(r'=+', '', text)\n",
    "    text = re.sub(r'-+', '', text)\n",
    "    text = re.sub(r'Table \\d+.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove parenthetical numbers and incomplete year references\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "    text = re.sub(r'(?<=\\d{4}),\\s*(?!\\d{4})', ' ', text)\n",
    "\n",
    "    # Clean up financial notation\n",
    "    text = re.sub(r'\\$\\s*', '$', text)\n",
    "    text = re.sub(r'(?<=\\d),(?=\\d{3})', '', text)\n",
    "\n",
    "    # Remove special formatting and headers\n",
    "    text = re.sub(r'\\s*\\(table\\s*of\\s*contents\\)\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Summary of.*?Table \\d+', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Standardize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences with more lenient filtering\"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "\n",
    "        # Skip sentences that:\n",
    "        if (\n",
    "            # More lenient length constraints\n",
    "            not (30 < len(s) < 500) or  # Changed from 40-400\n",
    "            len(s.split()) < 6 or       # Changed from 8\n",
    "            len(s.split()) > 60 or      # Changed from 50\n",
    "\n",
    "            # Basic pattern matching (removed many constraints)\n",
    "            re.match(r'^(table|figure)\\s+\\d+', s.lower()) or\n",
    "            'Amount Percent' in s or\n",
    "\n",
    "            # Structure checks (simplified)\n",
    "            s.count('$') > 5 or         # Changed from 3\n",
    "            re.search(r'\\d{6,}', s)     # Only filter very long numbers\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        cleaned_sentences.append(s)\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "# Process DAPT data for sentence-level training\n",
    "all_sentences = []\n",
    "sentence_pairs = []  # For tracking consecutive sentences\n",
    "\n",
    "print(\"Processing MDAs into sentences...\")\n",
    "for _, row in tqdm(dapt_data.iterrows(), total=len(dapt_data), desc=\"Processing MDAs\"):\n",
    "    # Process current MDA\n",
    "    current_sentences = split_into_sentences(row['current_mda_content'])\n",
    "\n",
    "    # Process next MDA\n",
    "    next_sentences = split_into_sentences(row['next_mda_content'])\n",
    "\n",
    "    # Add all sentences to main list\n",
    "    all_sentences.extend(current_sentences)\n",
    "    all_sentences.extend(next_sentences)\n",
    "\n",
    "    # Create pairs of consecutive sentences (for contextual similarity)\n",
    "    for doc_sentences in [current_sentences, next_sentences]:\n",
    "        for i in range(len(doc_sentences) - 1):\n",
    "            # Only create pairs if both sentences are meaningful\n",
    "            if len(doc_sentences[i].split()) >= 8 and len(doc_sentences[i+1].split()) >= 8:\n",
    "                sentence_pairs.append({\n",
    "                    'sentence1': doc_sentences[i],\n",
    "                    'sentence2': doc_sentences[i + 1],\n",
    "                    'company': row['company_name'],\n",
    "                    'filing_date': row['current_filing_date']\n",
    "                })\n",
    "\n",
    "# Create DataFrames\n",
    "sentences_df = pd.DataFrame({\n",
    "    'sentence': all_sentences\n",
    "})\n",
    "\n",
    "pairs_df = pd.DataFrame(sentence_pairs)\n",
    "\n",
    "# Modified paths for output\n",
    "output_base = r'C:\\Users\\abbra\\Documents\\Research Code\\Koval Paper\\Data\\Output\\Model Data\\DAPT Data'\n",
    "os.makedirs(output_base, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "\n",
    "sentences_df.to_csv(os.path.join(output_base, 'dapt_sentences.csv'), index=False)\n",
    "pairs_df.to_csv(os.path.join(output_base, 'dapt_sentence_pairs.csv'), index=False)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total individual sentences: {len(sentences_df)}\")\n",
    "print(f\"Total sentence pairs: {len(pairs_df)}\")\n",
    "\n",
    "# Print some quality checks\n",
    "print(\"\\nQuality Check:\")\n",
    "print(\"Average sentence length:\", sum(len(s.split()) for s in all_sentences) / len(all_sentences))\n",
    "print(\"Shortest sentence:\", min(all_sentences, key=len))\n",
    "print(\"Longest sentence:\", max(all_sentences, key=len))\n",
    "\n",
    "# Sample random sentences\n",
    "print(\"\\nRandom Sample of Clean Sentences:\")\n",
    "sample_sentences = random.sample(all_sentences, min(5, len(all_sentences)))\n",
    "for i, sentence in enumerate(sample_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "# Previous Dataset Statistics:\n",
    "# Total individual sentences: 4129608\n",
    "# Total sentence pairs: 4111984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Research/Paper Implementations/Koval Paper/Data/train_data.csv')\n",
    "val_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Research/Paper Implementations/Koval Paper/Data/val_data.csv')\n",
    "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Research/Paper Implementations/Koval Paper/Data/test_data.csv')\n",
    "\n",
    "# Convert dates to datetime\n",
    "def print_data_stats(df, name):\n",
    "    # Convert date column to datetime\n",
    "    df['current_filing_date'] = pd.to_datetime(df['current_filing_date'])\n",
    "\n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    print(f\"Number of samples: {len(df)}\")\n",
    "    print(f\"Date range: {df['current_filing_date'].min()} to {df['current_filing_date'].max()}\")\n",
    "    print(f\"Number of unique companies: {df['company_name'].nunique()}\")\n",
    "    print(f\"Average MDA length (current): {df['current_mda_content'].str.len().mean():.0f} chars\")\n",
    "    print(f\"Average MDA length (next): {df['next_mda_content'].str.len().mean():.0f} chars\")\n",
    "\n",
    "# Print statistics for each dataset\n",
    "print_data_stats(train_data, \"Training Data\")\n",
    "print_data_stats(val_data, \"Validation Data\")\n",
    "print_data_stats(test_data, \"Test Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
